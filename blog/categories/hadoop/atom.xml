<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hadoop | /var/log/aso.log]]></title>
  <link href="http://yoshihitoaso.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://yoshihitoaso.github.io/"/>
  <updated>2014-08-26T21:47:43+09:00</updated>
  <id>http://yoshihitoaso.github.io/</id>
  <author>
    <name><![CDATA[Yoshihito Aso]]></name>
    <email><![CDATA[yoshihito.asoh@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ubuntu上にCDH4（Hadoop）擬似分散モード環境を構築する （※おまけでHiveをインストール）]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/20/cdh4-pseudo-distributed-mode/"/>
    <updated>2014-08-20T15:44:13+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/20/cdh4-pseudo-distributed-mode</id>
    <content type="html"><![CDATA[<p>Ubuntuサーバ上にCDH4を擬似分散モードでインストールした時のメモ。</p>

<p>※参考情報：OSのバージョン
<code>
$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=12.10
DISTRIB_CODENAME=quantal
DISTRIB_DESCRIPTION="Ubuntu 12.10"
</code></p>

<!-- more -->


<h2>Oracle-Javaのインストール</h2>

<p>Java7をインストールする。
<code>
$ sudo add-apt-repository ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get install oracle-java7-installer
</code></p>

<p>バージョンの確認
<code>
$ java -version
java version "1.7.0_51"
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)
$ javac -version
javac 1.7.0_51
</code>
pathにJAVA_HOMEを設定しておく必要がある。
.bashrcに以下の設定を追記。
<code>
export JAVA_HOME=/usr/lib/jvm/java-7-oracle
export PATH=$PATH:$JAVA_HOME/bin
</code></p>

<h2>CDH4のインストール</h2>

<pre><code>$ wget http://archive.cloudera.com/cdh4/one-click-install/precise/amd64/cdh4-repository_1.0_all.deb
$ sudo dpkg -i cdh4-repository_1.0_all.deb
$ curl -s http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh/archive.key | sudo apt-key add -

$ sudo apt-get update
$ sudo apt-get install hadoop-conf-pseudo
</code></pre>

<p>ディレクトリを確認。各種設定ファイルが格納されている。
通常起動の場合は編集の必要はない。</p>

<pre><code>$ ls -l /etc/hadoop/conf.pseudo
total 40
-rw-r--r-- 1 root hadoop 1458 Feb 26 09:54 core-site.xml
-rw-r--r-- 1 root hadoop 1364 Feb 26 09:54 hadoop-env.sh
-rw-r--r-- 1 root hadoop 2890 Feb 26 09:54 hadoop-metrics.properties
-rw-r--r-- 1 root hadoop 1875 Feb 26 09:54 hdfs-site.xml
-rw-r--r-- 1 root hadoop 8735 Feb 26 10:21 log4j.properties
-rw-r--r-- 1 root hadoop 1549 Feb 26 09:54 mapred-site.xml
-rw-r--r-- 1 root hadoop 1104 Feb 26 09:54 README
-rw-r--r-- 1 root hadoop 2361 Feb 26 09:54 yarn-site.xml
</code></pre>

<h2>HDFS /tmp directory を作成する</h2>

<p>hdfsユーザでHDFSのフォーマットを実施する。
<code>
$ sudo -u hdfs
$ hdfs namenode -format
</code></p>

<p>HDFSを起動。
<code>
$ /etc/init.d/hadoop-hdfs-namenode
$ /etc/init.d/hadoop-hdfs-datanode
$ /etc/init.d/hadoop-hdfs-secondarynamenode
</code>
とすれば良いけど、毎回このコマンドを打つのは面倒なので以下の様なスクリプト（hadoop-hdfs-start）を作っておく。
&#8220;`</p>

<h1>!/bin/bash</h1>

<p>for service in /etc/init.d/hadoop-hdfs-*
do
sudo $service start
done
&#8220;`
停止の場合も同様。以下の様なスクリプト（hadoop-hdfs-stop）を作っておくとラク。</p>

<pre><code>#!/bin/bash
for service in /etc/init.d/hadoop-hdfs-*
do
sudo $service stop
done
</code></pre>

<p>起動を実行したら、各サービスが起動していることを確認する。
<code>
$ sudo $JAVA_HOME/bin/jps
4233 Jps
2476 NodeManager
2007 SecondaryNameNode
2622 ResourceManager
1827 NameNode
1652 DataNode
</code></p>

<h2>必要なDirectoryを作成する</h2>

<p>HDFS上に必要なディレクトリを作成していく。この作業もhdfsユーザで実施する。</p>

<pre><code>$ sudo su - hdfs
$ hadoop fs -mkdir /tmp　←HDFS上に/tmpを作成
$ hadoop fs -chmod -R 1777 /tmp
$ hadoop fs -mkdir /var/log/hadoop-yarn
$ hadoop fs -chown yarn:mapred /var/log/hadoop-yarn
$ hadoop fs -mkdir /tmp/hadoop-yarn/staging
$ hadoop fs -chmod -R 1777 /tmp/hadoop-yarn/staging
$ hadoop fs -mkdir /tmp/hadoop-yarn/staging/history/done_intermediate
$ hadoop fs -chmod -R 1777 /tmp/hadoop-yarn/staging/history/done_intermediate
$ hadoop fs -mkdir /user/$USER　←hdfsユーザのホームディレクトリ/user/hdfs
$ hadoop fs -chown hdfs /user/$USER
</code></pre>

<p>/user/hdfsが作業ディレクトリと考えてよい。上記実行中エラーが出て中断するようなら、HDFSが落ちている可能性があるので起動状態を確認してみる。</p>

<p>HDFS上のlsは以下のようにして実行する。上記で作ったtmp,user,varができてればOK。
<code>
$ hadoop fs -ls /
Found 3 items
drwxrwxrwt – hdfs supergroup 0 2013-01-05 12:10 /tmp
drwxr-xr-x – hdfs supergroup 0 2013-01-05 12:36 /user
drwxr-xr-x – hdfs supergroup 0 2013-01-05 12:09 /var
</code></p>

<h2>Start YARN !!!!!</h2>

<p>管理ユーザにスイッチしてYARN (MapReduce)を起動する。
<code>
$ sudo service hadoop-yarn-resourcemanager start
$ sudo service hadoop-yarn-nodemanager start
$ sudo service hadoop-mapreduce-historyserver start
</code></p>

<p>プロセスの確認。以下の様な感じになっていればOK。</p>

<pre><code>$ sudo $JAVA_HOME/bin/jps
2598 ResourceManager
4798 JobHistoryServer
4840 Jps
1752 NameNode
2448 NodeManager
2043 SecondaryNameNode
1496 DataNode
</code></pre>

<h1>おまけ）Hiveをインストールして使ってみる</h1>

<h2>Hiveのインストール</h2>

<p>以下の手順でHiveをインストールする。
<code>
$ cd Downloads/
$ mkdir hive
$ cd hive/
$ wget http://mirror.tcpdiag.net/apache/hive/stable/hive-0.11.0.tar.gz
$ tar xzf hive-0.11.0.tar.gz
$ mkdir /usr/lib/hive
$ mv hive-0.11.0 /usr/lib/hive/hive-0.11.0
</code></p>

<p>環境変数を設定する。
<code>
$ cd
$ vim .bashrc
$ export HIVE_HOME=/usr/lib/hive/hive-0.11.0
$ export PATH=$PATH:$HIVE_HOME/bin
</code></p>

<h3>Login</h3>

<pre><code>$ hive
...
..
.

hive&gt;
</code></pre>

<p>でOK。</p>

<h2>Sample : Create Table</h2>

<p>Tableを作成して、CSV形式のファイルをロードしてみる。
<code>
hive&gt; create table cats (id int, name string, birthday string) row format delimited fields terminated by ',' lines terminated by '\n';
</code></p>

<p>csv形式のファイルを以下のように用意する。
&#8220;`
$ vim cats.csv</p>

<p>1,Tama,19929423
2,Tora,19930304
3,Nya,19930304</p>

<pre><code>
以下のコマンドでロード出来る。
</code></pre>

<p>hive> load data local inpath &lsquo;/root/hadoop/hive/cats.csv&rsquo; overwrite into table cats;
&#8220;`</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Conference Japan 2014　参加メモ #hcj2014]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/08/hadoop-conference-japan-2014-number-hcj2014/"/>
    <updated>2014-08-08T21:49:15+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/08/hadoop-conference-japan-2014-number-hcj2014</id>
    <content type="html"><![CDATA[<p><img src="https://s3-ap-northeast-1.amazonaws.com/y-asoh.aws-demo.net/logo.png" title="hadoop" alt="hadoop" /></p>

<p>メモは残してそのままにしていたが、こちらに書いておくことにする。</p>

<!-- more -->


<p>Hadoop Conference Japan 2014 に参加してきたので、その参加メモ。</p>

<p>7/8に開催された Hadoop Conference Japan 2014に参加してきた（去年に引き続き2回目の参加）。</p>

<p>そもそもHadoop Conference Japanとは、並列分散処理フレームワーク Apache Hadoop および周辺のオープンソースソフトウェアに関するユーザカンファレンス。
日本Hadoopユーザー会の有志によって運営されており、今回で5回目の開催になる。</p>

<p>↓各種レポート記事へのリンク↓</p>

<ul>
<li><strong>gihyo.jp</strong>

<ul>
<li><a href="http://gihyo.jp/news/report/2014/07/0902">日本よ，これが2014年のHadoopだ！─「Hadoop Conference Japan 2014」基調講演レポート</a></li>
</ul>
</li>
<li><strong>ITpro</strong>

<ul>
<li><a href="http://itpro.nikkeibp.co.jp/article/NEWS/20140708/569985/">「Hadoopはビッグデータの“OSカーネル”」、Hadoop Conference Japan開催</a></li>
<li><a href="http://itpro.nikkeibp.co.jp/article/COLUMN/20140711/570863/">NTTデータが4000コアのクラスターでSparkを試行、NTTドコモからの要望受け</a></li>
</ul>
</li>
<li><strong>Publickey</strong>

<ul>
<li><a href="http://www.publickey1.jp/blog/14/yarnhadoophadoop_conference_japan_2014.html">YARNの登場によりHadoopは複数の並列分散処理エンジンを併用できる環境へ</a></li>
<li><a href="http://www.publickey1.jp/blog/14/hadoopoltphadoop_conference_japan_2014.html">HadoopはいずれOLTPも実現し、エンタープライズデータハブとなる</a></li>
</ul>
</li>
</ul>


<h1>タイムテーブル</h1>

<p>当日のタイムテーブルは<a href="https://www.eventbrite.com/e/hadoop-conference-japan-2014-tickets-12016613013">こちら</a></p>

<p>午前のキーノートから始まり、昼食（無料！）を挟んで、午後はHadoopに関する技術トピックや活用事例を紹介する発表、という流れ。</p>

<p>以下は、自分が聞いた発表</p>

<p>【午前：キーノート】</p>

<ul>
<li>米谷 修 （リクルートテクノロジーズ）、濱野 賢一朗 （日本Hadoopユーザー会, NTTデータ）</li>
<li>Doug Cutting （Hadoop生みの親, Apache Software Foundation, Cloudera） 『The Future of Data』</li>
<li>Patrick Wendell （Apache Spark主要開発者, Databricks） 『The Future of Spark』 [講演資料]</li>
<li>太田 一樹 （Treasure Data CTO） 『Hadoopエコシステムの変遷と、見えてきた使いどころ』</li>
</ul>


<p>【午後】</p>

<ul>
<li>リクルート式Hadoopの使い方 3rd Edition : 石川 信行（リクルートテクノロジーズ）</li>
<li>SQLによるバッチ処理とストリーム処理 : 田籠 聡 (LINE)</li>
<li>A Deeper Understanding of Spark Internals : Patrick Wendell （Databricks）</li>
<li>Spark1.0での動作検証 - Hadoopユーザ・デベロッパから見たSparkへの期待 : 土橋 昌 （NTTデータ）</li>
<li>Treasure Data on The YARN : 小林 隆（Treasure Data）</li>
<li>並列SQLエンジンPresto - 大規模データセットを高速にグラフ化する方法 : 古橋 貞之（Treasure Data）</li>
</ul>


<h1>午前：キーノート</h1>

<h2>濱野 賢一朗 （日本Hadoopユーザー会, NTTデータ）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36781484" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/hamaken/hadoop-conference-japan-2014-36781484" title="Hadoop Conference Japan 2014 ご挨拶・Hadoopを取り巻く環境" target="_blank">Hadoop Conference Japan 2014 ご挨拶・Hadoopを取り巻く環境</a> </strong> from <strong><a href="http://www.slideshare.net/hamaken" target="_blank">hamaken</a></strong> </div></p>

<ul>
<li>今回の参加者数は1296名。そのうち約65%が初参加とのこと。</li>
<li>Hadoopは絶賛進化中。現在は2.X系の開発が中心</li>
<li>その中心的なトピックは「YARN」（※）</li>
<li>日本におけるHadoopの利用ユーザは徐々に増えており、今回の参加者の44%以上が6ヶ月以上の利用経験を有する</li>
<li>利用しているエコシステムとしてはHive、HBaseが多い。Impala、Sparkなどの比較的新しいプロダクトについても利用ユーザは相当数いる。</li>
</ul>


<blockquote><p><strong>YARN（Yet Another Resource Negotiator）についての参考資料</strong></p>

<ul>
<li>Apache Hadoop 2.4.1 - YARN <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html</a></li>
<li>YARN の紹介 <a href="http://www.ibm.com/developerworks/jp/bigdata/library/bd-yarn-intro/">http://www.ibm.com/developerworks/jp/bigdata/library/bd-yarn-intro/</a></li>
</ul>
</blockquote>

<h2>Doug Cutting （Hadoop生みの親、Apache Software Foundation, Cloudera）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36826369" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/Cloudera_jp/the-future-of-data-jp" title="The future of data by Doug Cutting #hcj2014" target="_blank">The future of data by Doug Cutting #hcj2014</a> </strong> from <strong><a href="http://www.slideshare.net/Cloudera_jp" target="_blank">Cloudera Japan</a></strong> </div></p>

<ul>
<li>ハードウェアの価格は更に安く、データの価値は更に高まる（よりハイレゾリューションに）、そのためのソフトウェアが必要になる</li>
<li>そしてオープンソースが勝ち残る (luceneは 1999 から開発開始しているが、事実生き残っている）</li>
<li>オープンソースは使う側から見てもリスクを低減する。プラットフォーム技術については特にOSSであることが必要とされる。</li>
<li>Hadoopの機能はさらに向上し、Hadoopが当たり前になるだろう</li>
<li>GoogleのSpannerのようにトランザクション処理をHadoop上でサポートするという話があった（※）</li>
</ul>


<blockquote><p><strong>※参考記事</strong></p>

<ul>
<li>Hadoop creator: &lsquo;Google is living a few years in the future and sending the rest of us messages&rsquo; | ZDNet <a href="http://www.zdnet.com/hadoop-creator-google-is-living-a-few-years-in-the-future-and-sending-the-rest-of-us-messages-7000023160/">http://www.zdnet.com/hadoop-creator-google-is-living-a-few-years-in-the-future-and-sending-the-rest-of-us-messages-7000023160/</a></li>
</ul>
</blockquote>

<h2>Patrick Wendell （Apache Spark主要開発者、 Databricks）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36856077" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/hadoopconf/the-future-of-apache-spark" title="The Future of Apache Spark" target="_blank">The Future of Apache Spark</a> </strong> from <strong><a href="http://www.slideshare.net/hadoopconf" target="_blank">Hadoop Conference Japan</a></strong> </div></p>

<ul>
<li>Spark開発者の一週間の活動

<ul>
<li>パッチの投稿や修正等：500件、JIRA/GitHub上でのコメント：200件、メール：140スレッド、マージされるパッチの数：80件</li>
</ul>
</li>
<li>Sparkはここ1年で急速に成長した</li>
<li>APIの安定性を重視している。APIを破壊するようなパッチはビルドに失敗するようにしている</li>
<li>開発者にやさしいリリースサイクルでリリースを行っている。マイナーリリースは3ヶ月毎、必要に応じてメンテナンスリリースを行っている。</li>
<li>Sparkの主要エコシステムはSpark SQL（SQL on Spark）、MLLib（機械学習ライブラリ）、GraphX（グラフDB）、Spark Streaming（ストリーム処理）</li>
<li>そのうちSpark SQLは他のコンポーネントよりも早く成長している。MLLibがその次。</li>
<li>SparkR といプロダクトもある。RでSparkを動かす。</li>
<li>Databricks Cloudのデモ。Web UI上でSparkのプログラムが書ける。SQLも実行できる。その実行結果を簡単にグラフ表示ができる。すごい。</li>
</ul>


<blockquote><p><strong>Apache Spark に関する参考資料</strong></p>

<ul>
<li>Apache Spark の紹介（前半：Sparkのキホン） <a href="http://www.slideshare.net/hadoopxnttdata/apache-spark-spark">http://www.slideshare.net/hadoopxnttdata/apache-spark-spark</a></li>
<li>Apache Sparkのご紹介 （後半：技術トピック） <a href="http://www.slideshare.net/hadoopxnttdata/apache-spark">http://www.slideshare.net/hadoopxnttdata/apache-spark</a></li>
<li>Spark: 高速なデータ分析のための新たな手段 <a href="http://www.ibm.com/developerworks/jp/opensource/library/os-spark/">http://www.ibm.com/developerworks/jp/opensource/library/os-spark/</a></li>
</ul>
</blockquote>

<h2>太田 一樹 （Treasure Data CTO）</h2>

<p><strong>『Hadoopエコシステムの変遷と、見えてきた使いどころ』（資料未公開）</strong></p>

<ul>
<li>ただ単に「安いストレージ的」に使うのであれば、GlusterFSやCephといったストレージ特化の様々なプロダクトがある。こちらのほうが優れてる。</li>
<li><p><strong>Hadoop エコシステムの進化と混沌</strong> 4つの側面で様々なプロダクトが乱立している</p>

<ul>
<li>1.Collect Any Types of data

<ul>
<li>様々なデータ収集ミドルウェアの開発が進んでいる</li>
<li>Fluentd, Kafka, Flume, Sqoop</li>
</ul>
</li>
<li>2.Store Any types of data economically

<ul>
<li>Parquet, ORCFile (format)</li>
<li>HDFS, HBase, Accumulo</li>
<li>Ambali, HUE, Cluudera Maager 管理/運用の支援は大事</li>
<li>Treasure Data, AWS EMR</li>
</ul>
</li>
<li>3.Faster Use of Data

<ul>
<li>いかに早くデータを扱うか？</li>
<li>YARN</li>
<li>Storm, Samza, Norikra</li>
<li>Apache Tez, Spark</li>
<li>HiveQL, Pig</li>
<li>Java: Cascading, Apache Crunch</li>
</ul>
</li>
<li>4.Better Use of Data

<ul>
<li>いかにうまくデータを扱うか？</li>
<li>SQL on Hadoop：Impala, SparkSQL, Presto, Drill</li>
<li>機械学習：Mahout, Spark MLlib, Hivemall</li>
</ul>
</li>
</ul>
</li>
<li><p>他の選択肢の進化</p>

<ul>
<li>  Database の進化

<ul>
<li>  MPP（Massiely Parallel Processing） MPPデータベース（RedShiftやBigQueryなど）　（※）</li>
<li>  Schema-on-Write （先にスキーマを決めないといけない） 。アドホックな解析をするためのデータベースとしては少し不向き。</li>
<li>  Oracle, DB2, SQLserver</li>
<li>  Teradata, Netezza, Vertica, ParAccel, Greenplum</li>
<li>  多くのベンダーが Hadoop 対応を表明</li>
<li>  基本的にはスキーマを決めないといけないが、Vertica Zonemap など Schema-on-Read 対応なども</li>
<li>  現在の主流

<ul>
<li>  Hadoop に生データをすべて集約</li>
<li>  そこから集計集約したデータを MPP データベースに保存</li>
</ul>
</li>
<li>  Hadoop は構造化データとの境界線に</li>
<li>  MPP データベースは非構造化データの領域に踏み込む</li>
<li>  誰がマーケットリーダーになっていくのか注視していかないといけない</li>
<li>  使用する側は一層の知識とトレンドの把握が必要。</li>
</ul>
</li>
</ul>
</li>
</ul>


<blockquote><p><strong>※参考資料</strong></p>

<p>MPPの使い分けの話</p>

<ul>
<li>MPP on Hadoop, Redshift, BigQuery - Go ahead! <a href="http://repeatedly.github.io/ja/2014/07/mpp-on-hadoop-redshift-bigquery/">http://repeatedly.github.io/ja/2014/07/mpp-on-hadoop-redshift-bigquery/</a></li>
</ul>
</blockquote>

<h1>午後</h1>

<h2>リクルート式Hadoopの使い方 3rd Edition : 石川 信行（リクルートテクノロジーズ）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36770927" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/recruitcojp/hadoop20140707" title="Hadoopカンファレンス20140707" target="_blank">Hadoopカンファレンス20140707</a> </strong> from <strong><a href="http://www.slideshare.net/recruitcojp" target="_blank">Recruit Technologies</a></strong> </div></p>

<ul>
<li>リクルートグループのHadoop活用事例に関する発表</li>
<li>様々な利用用途でHadoop、およびHadoopのエコシステムを利用している。詳細は資料の通り。</li>
<li>リクルートでは2010年からHadoopの研究開発、2011年から本格展開を始めた</li>
<li>2014年はアドホック分析基盤の導入等の活動を行っている</li>
<li>導入に際しては、既存サービスを生かしつつ、徐々に利用範囲を広げていっている（安く、早く提供）</li>
<li>本番提供しているサービスの他にも、現在も様々な技術検証を並行して実施している。

<ul>
<li>画像解析（スパースコーディングなど）</li>
<li>テキスト解析（Skip-Gramなど）</li>
<li>グラフ（Titanなど）</li>
</ul>
</li>
</ul>


<h2>並列SQLエンジンPresto - 大規模データセットを高速にグラフ化する方法 : 古橋 貞之（Treasure Data）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36771514" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/frsyuki/presto-hadoop-conference-japan-2014" title="Presto - Hadoop Conference Japan 2014" target="_blank">Presto - Hadoop Conference Japan 2014</a> </strong> from <strong><a href="http://www.slideshare.net/frsyuki" target="_blank">Sadayuki Furuhashi</a></strong> </div></p>

<ul>
<li><p>1.イントロ</p>

<ul>
<li>PrestoとはGBからPBのデータ分析を対話的に行う分散SQLクエリエンジン</li>
<li>2012秋からFacebookで開発された</li>
<li>Prestoが解決しようとする問題は

<ul>
<li>BIツールから直接HDFS上のデータを可視化できない</li>
<li>日次バッチでPostgreSQLやRedshiftにデータを入れる必要がある</li>
<li>いくつかのデータはHDFS上にはない。そのため分析時にHDFS上にコピーが必要</li>
</ul>
</li>
<li>Prestoだとクエリをミリ秒から分で処理。ただしETLにはMapReduceやHiveは必要</li>
<li>BIツールから接続可能。ODBC/JDBCコネクタ</li>
<li>複数のデータソースにまたがってクエリを実行可能</li>
</ul>
</li>
<li><p>2.分散アーキテクチャ</p>

<ul>
<li>Discovery Service</li>
<li>Coordinator</li>
<li>Worker</li>
<li><p>Connector Plugin : ConnectorはJavaで書かれており、ストレージとメタデータの実装。自作も可能。</p>

<ul>
<li>Hive Connector</li>
<li>Cassandra Connector</li>
<li>MySQL through JDBC Connector(prerelease)</li>
</ul>
</li>
<li><p>Coordinator HA構成を組める</p></li>
<li>BIツールにはODBC/JDBCドライバが必要。ただ一から実装は大変。ということで“Prestogres”を作った</li>
</ul>
</li>
<li><p>3.クエリ実行</p>

<ul>
<li>Presto自身はデータベースではない。データストアに対してSQLを発行するエンジン（MapReduceではない）</li>
<li>実行モデルはDAGベース</li>
<li>全てのタスクはパラレルに実行され、データはメモリからメモリに引き渡される</li>
</ul>
</li>
<li><p>4.モニタリング・設定</p>

<ul>
<li>モニタリングが充実している。JMX HTTP APIで詳細な情報を取得できる。</li>
</ul>
</li>
<li><p>5.ロードマップ</p></li>
<li><p>QA</p>

<ul>
<li>同様なプロダクトであるImpalaとの性能比較

<ul>
<li>現状ではImpalaの方が速い</li>
<li>Impalaとの違いは、クエリが落ちてもプロセスが落ちない</li>
<li>ログが取りやすい。Prestoは運用周りがよく考慮されている</li>
<li>開発体制がオープン。pullreqもすぐに取り込まれる</li>
</ul>
</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
</feed>
