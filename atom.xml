<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[/var/log/aso.log]]></title>
  <link href="http://yoshihitoaso.github.io/atom.xml" rel="self"/>
  <link href="http://yoshihitoaso.github.io/"/>
  <updated>2014-09-01T13:53:18+09:00</updated>
  <id>http://yoshihitoaso.github.io/</id>
  <author>
    <name><![CDATA[Yoshihito Aso]]></name>
    <email><![CDATA[yoshihito.asoh@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ubuntu に JAVA7 (OpenJDK) をインストールする]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/09/01/ubuntu-jdk-install/"/>
    <updated>2014-09-01T13:48:57+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/09/01/ubuntu-jdk-install</id>
    <content type="html"><![CDATA[<p>UbuntuにJDKをセットアップする方法をメモ。</p>

<p>※参考情報：OSのバージョン</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat /etc/lsb-release
</span><span class='line'>DISTRIB_ID=Ubuntu
</span><span class='line'>DISTRIB_RELEASE=12.10
</span><span class='line'>DISTRIB_CODENAME=quantal
</span><span class='line'>DISTRIB_DESCRIPTION="Ubuntu 12.10"</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>OpenJDK のパッケージを検索</h2>

<p>インストールを行うパッケージを検索する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo apt-cache search openjdk
</span><span class='line'>...
</span><span class='line'>openjdk-7-jdk - OpenJDK Development Kit (JDK)
</span><span class='line'>openjdk-7-jre - OpenJDK Java runtime, using Hotspot JIT
</span><span class='line'>openjdk-7-jre-headless - OpenJDK Java runtime, using Hotspot JIT (headless)
</span><span class='line'>openjdk-7-jre-zero - Alternative JVM for OpenJDK, using Zero/Shark
</span><span class='line'>openjdk-7-source - OpenJDK Development Kit (JDK) source files
</span><span class='line'>...</span></code></pre></td></tr></table></div></figure>


<p>ここでは、openjdk-7-jdk、openjdk-7-jre、openjdk-7-source をインストールすることとする。</p>

<h2>インストール</h2>

<p>以下のとおりインストールする。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo apt-get install openjdk-7-jdk
</span><span class='line'>$ sudo apt-get install openjdk-7-jre
</span><span class='line'>$ sudo apt-get install openjdk-7-source</span></code></pre></td></tr></table></div></figure>


<h2>JAVA のバージョンを確認</h2>

<p>バージョンの確認を行う。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ java -version
</span><span class='line'>java version "1.7.0_65"
</span><span class='line'>OpenJDK Runtime Environment (IcedTea 2.5.1) (7u65-2.5.1-4ubuntu1~0.12.04.2)
</span><span class='line'>OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)
</span><span class='line'>javac 1.7.0_65</span></code></pre></td></tr></table></div></figure>


<h2>環境変数の設定</h2>

<p>環境変数（JAVA_HOME）の設定は以下のように行った。Pathは若干異なる場合があると思う。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ echo "export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64/\"" &gt;&gt; ~/.bashrc
</span><span class='line'>$ source ~/.bashrc</span></code></pre></td></tr></table></div></figure>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YAPC Asia Tokyo 2014 (DAY2) 参加メモ]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/31/yapc-asia-tokyo-2014-day2/"/>
    <updated>2014-08-31T14:05:06+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/31/yapc-asia-tokyo-2014-day2</id>
    <content type="html"><![CDATA[<p>8/29-30に開催された<a href="http://yapcasia.org/2014/">YAPC::Asia Tokyo 2014</a> に参加して来ました。</p>

<p>こちらは8/30分の参加メモです。</p>

<p>※補足の参考資料等、まだ書ききれていない部分があるので、今後追記していきます。</p>

<!-- more -->


<h2>Dockerで遊んでみよっかー (Masahiro Nagano / LINE)</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/38506505" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/kazeburo/docker-yapcasia-tokyo-2014" title="Dockerで遊んでみよっかー YAPC::Asia Tokyo 2014" target="_blank">Dockerで遊んでみよっかー YAPC::Asia Tokyo 2014</a> </strong> from <strong><a href="//www.slideshare.net/kazeburo" target="_blank">Masahiro Nagano</a></strong> </div></p>

<p>DockerのコントロールはVagrantからやるのがオススメっぽいので、今度からはそうしようと思います。</p>

<p>Dockerの「速い起動速度」と「揮発性」を意識して、色々な利用用途が考えられていますが、紹介されていたもので以下はなかなか良いですね。</p>

<blockquote><p><a href="http://blog.linknode.net/article/1404955273">Docker コンテナで MySQL を使ったテストの高速化 - stfuawsc</a></p></blockquote>

<p>自分自身、ちょっと触ってみてやめてしまった経緯があるのですが、ググってみると見ないうちにかなり色々とアイデアが出てきているので、もう一度利用方法を考えてみようと思ってます。</p>

<blockquote><p><a href="http://blog.nomadscafe.jp/2014/06/plenvxbuildperl-build.html">plenvやxbuildで使っているperl-buildを信頼性の向上目指してアップデートしました - blog.nomadscafe.jp</a></p></blockquote>

<h2>Google BigQuery で DWH 構築 (Naoya Ito / KAIZEN)</h2>

<script async class="speakerdeck-embed" data-id="5c680810121d0132c86f02e2e0c65448" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>


<p>満員御礼でした。注目度が高いんですね。</p>

<p>ただ、現在既に使ったことがある人はあまりいなかったようなので、これから使うといった感じなんですかね？</p>

<p>自分も使いどころを模索して（※単純なDWH的な使い方は普通にできるとは思う）、少し触ってみたり、<a href="http://www.amazon.co.jp/Google-BigQuery-Analytics-Jordan-Tigani-ebook/dp/B00JUUZIZI">&ldquo;Google BigQuery Analytics&rdquo;</a>を読んでみたりしています。</p>

<p>9章の内部処理のところはまだ読んでないので、先にちょっとだけ話を聞くことが出来て良かったです。</p>

<!--
##Perl for Perl Mongers (YAPCに来た人は皆Perl Mongerでは？)
資料： http://songmu.github.io/slides/yapc-asia2014/#0

※まとめ中
-->


<h2>そんなにビッグでもないデータ処理手法の話 (@tagomoris / LINE)</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/38510397" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/tagomoris/handling-not-so-big-data" title="Handling not so big data" target="_blank">Handling not so big data</a> </strong> from <strong><a href="//www.slideshare.net/tagomoris" target="_blank">SATOSHI TAGOMORI</a></strong> </div></p>

<p>データ解析の流れは大きく4つのフェーズに分けられます。</p>

<ul>
<li>収集</li>
<li>加工・格納</li>
<li>処理</li>
<li>可視化</li>
</ul>


<p>今回の話は“処理（Processing）”の話でした。つまり、蓄えられているデータをどう処理するか？というところの話。</p>

<p>Naoya Itoさんの話でも出てきていましたが、この分野は以前（5年前とか？）に比べて選択肢が多様化しており、目的別に様々なプロダクトが出てきています。</p>

<ul>
<li>Large Batch

<ul>
<li>Hadoop（＋Hive、Pig）、Spark、Tez</li>
</ul>
</li>
<li>Short Batch

<ul>
<li>MPP Engines: BigQuery, Presto, Apache Impala</li>
</ul>
</li>
<li>Stream Processing

<ul>
<li>Storm, Norikra</li>
</ul>
</li>
</ul>


<p>これらProcessingにおける多様化に関する話でした。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[YAPC Asia Tokyo 2014 (DAY1) 参加メモ]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/29/yapc-asia-tokyo-2014/"/>
    <updated>2014-08-29T11:02:38+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/29/yapc-asia-tokyo-2014</id>
    <content type="html"><![CDATA[<p>8/29-30に開催された<a href="http://yapcasia.org/2014/">YAPC::Asia Tokyo 2014</a> に参加して来ました。</p>

<p>こちらは8/29分の参加メモです。</p>

<p>※補足の参考資料等、まだ書ききれていない部分があるので、今後追記していきます。</p>

<!-- more -->


<h2>インフラエンジニア（狭義）は死んだ (Satoshi Suzuki / LINE)</h2>

<p>（資料未公開）</p>

<ul>
<li>昨今のインフラエンジニアはコードが書ける・読めることが重要視されている</li>
<li>Nagios、Fluentdなどなどインフラ・運用管理のコード化を行うためのOSSは様々存在するが、それらはプラグインなどを自前で書いたりすると最大限の恩恵を受けることが出来る。</li>
<li>目の前にある「解決すべき問題・課題」は何か？それを解決するためにCode化する。</li>
<li>インフラのコード化とはApache、MySQLのソースコードレベルのチューニングというわけではない。</li>
<li>課題を見つけるためにCloudForecast、GrowthForecast・・・など監視の仕組みを構築することも必要。</li>
<li>従来型のインフラエンジニアの需要が無くなるかというとそうではないだろう</li>
<li>“インフラエンジニア”に対する先入観を無くす必要があるだろう（自分はインフラエンジニアだから・・・とか）</li>
</ul>


<h2>Go For Perl Mongers　(Daisuke Maki / LINE)</h2>

<p>資料： <a href="http://go-talks.appspot.com/github.com/lestrrat/go-slides/2014-yapcasia-go-for-perl-mongers/main.slide#1">Go For Perl Mongers</a></p>

<p>詳しいことは資料に書かれているので、以下は抜粋。</p>

<p>GoはLLっぽいCだと思っている。ただ、コンパイル言語である。LLっぽいがそうではない。</p>

<h4>例外とエラー</h4>

<ul>
<li>例外が存在しない。

<ul>
<li>panic(), recover()は基本的に使わない。</li>
<li>エラーを戻り値に入れて早めにエラー処理（return/continue/break）するというのがセオリー。</li>
<li>後続処理の大前提になっているような場合にはpanic()。ファイル存在チェックとか。</li>
</ul>
</li>
</ul>


<h4>構造体設計</h4>

<ul>
<li>基本はCのstruct</li>
<li>オブジェクト指向ではない。型階層・継承がない。</li>
<li>継承ではなく、委譲

<ul>
<li>考え方を変える(1) - 継承せずに小さな部品で合成する</li>
<li>考え方を変える(2) - ラップする</li>
<li>考え方を変える(3) - 類似構造体の関係性</li>
</ul>
</li>
</ul>


<h4>並行処理</h4>

<ul>
<li>goroutineはスレッドのイメージ。実行はgoのランタイム任せでどのスレッドでいつ動作するかは制御できない</li>
<li>並行・並列処理は落とし穴だらけ。Goを使えば簡単にできるはウソ。</li>
</ul>


<h2>最近のウェブサービスの検索機能やその先の話　(Yusuke Yanbe/Hatena)</h2>

<h4>はてなブックマークの検索の歴史について</h4>

<ul>
<li>2007 MySQL（LIKE検索）

<ul>
<li>特に問題はなかった</li>
</ul>
</li>
<li>2008 サービス規模が拡大

<ul>
<li>過去のナレッジを有効に活用できるようにしたい</li>
<li>Sedue を導入して過去エントリの全文検索が可能に</li>
</ul>
</li>
<li>2009 関連エントリ

<ul>
<li>タグを元に関連エントリを表示できるようになった</li>
<li>Senna で全文検索結果にまとめてタグづけなど可能に</li>
<li>スケールしないので課金ユーザのみ</li>
</ul>
</li>
<li>2011 検索機能が不調になる

<ul>
<li>保守を新たに結ぶか、OSS を利用するか</li>
<li>Solrの利用を検討</li>
</ul>
</li>
<li>2013 Elasticsearch検討

<ul>
<li>今は多くの場所で使っている</li>
</ul>
</li>
</ul>


<h4>Elasticsearchの特徴</h4>

<ul>
<li>複雑なクエリを記述できる

<ul>
<li>例: 自分のブクマから YAPC が入ってるエントリを検索</li>
<li>例: お気に入りから YAPC エントリを検索</li>
</ul>
</li>
<li>はてなブックマークカウンター

<ul>
<li>ウェブサイト全体のブックマーク数を表示する</li>
<li>URL で検索して足し合わせていたが、aggregation で実現できる</li>
</ul>
</li>
</ul>


<h4>Elasticsearchをなぜ採用したか？</h4>

<ul>
<li>Query DSLの自由度で高度な企画要件にも耐えうる</li>
<li>RDBMSだとスケールしにくい処理がオフロード可能</li>
</ul>


<h4>質疑応答</h4>

<p>Q：オンラインでマッピングを変えるような時にどうしてるか？</p>

<p>→　A：同じ構成の ES を用意して切り替える。ブルーグリーンデプロイメント。</p>

<h2>Scala In Perl Company (hakobe/Hatena)</h2>

<script async class="speakerdeck-embed" data-id="660d60b0116e0132712f66e2202859ae" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>


<h4>継続的なソフトウェア進化</h4>

<p>なぜPerlを使うか？</p>

<ul>
<li>少ない記述量で多くのことが出来る</li>
<li>CPANライブラリが豊富　など</li>
</ul>


<p>10年の歳月が過ぎて状況が変わってきた。価値を生み出し続けるには、ソフトウェア進化を継続していかなければいけない。</p>

<ul>
<li>機能追加</li>
<li>リニューアル</li>
<li>パフォーマンス改善</li>
</ul>


<h4>「Perlで大規模開発」の難しさ</h4>

<p>安全に機能のカスタマイズをしていくために様々な工夫を行ってきた。</p>

<ul>
<li>テストによる保護</li>
<li>CI</li>
<li>ソースコード解析</li>
<li>開発フローの最適化　など</li>
</ul>


<p>それでもエラー検知に限界があるし、テストにそこまでコストをかけられないという現実問題がある。</p>

<p>つまりPerlでは、「継続的なソフトウェア進化」における難しさがあった。</p>

<h4>Scala採用の経緯</h4>

<p>macherel（はてなの提供するサーバ管理サービス）という新サービスを開発することになった。</p>

<p>言語の採用に自由度があったため、紆余曲折経てScalaを採用。</p>

<h4>Scalaの良い/つらいと感じるところ</h4>

<p>良いと感じるところ</p>

<ul>
<li>コードの変更が安心</li>
<li>レビューが安心（機能に集中出来る）　など</li>
</ul>


<p>つらいと感じるところ</p>

<ul>
<li>コンパイル時間が長すぎる</li>
<li>言語機能が豊富すぎて学習コストが高い　など</li>
</ul>


<!--
## One layer down below. (Kang-min Liu/Booking.com)

(資料未公開)

※まとめ中

hijkというHttpクライアントを作った
perlコードで200行くらいのシンプルなクライアント
Elasticsearchを利用するために作った

hijkはlwpの12倍速いというBenchmark結果

なぜ必要だったか
とにかく性能を上げる
Http/1.1のプロトコルで出来ることは様々
app←→databaseで通信を行うことだけが必要であったため、必要な機能に絞った

その他にも「必要な」OSSを作っている

-->


<h2>WHERE狙いのキー、ORDER BY狙いのキー (yoku0825)</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/38479437" width="425" height="355" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="//www.slideshare.net/yoku0825/whereorder-by" title="Where狙いのキー、order by狙いのキー" target="_blank">Where狙いのキー、order by狙いのキー</a> </strong> from <strong><a href="//www.slideshare.net/yoku0825" target="_blank">yoku0825</a></strong> </div></p>

<p>※RDBMSのSQL（INDEX）チューニングに関する話。資料に書いてある通りなので、追加のメモ等は特に無し。</p>

<!--
## Java For Perl Mongers Kazuhiro Osawa

※まとめ中
-->

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PythonからHiveServer2に接続する方法]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/27/pyhs2/"/>
    <updated>2014-08-27T15:15:50+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/27/pyhs2</id>
    <content type="html"><![CDATA[<p>PythonからHiveQLを実行したかったので、HiveServerを利用しました。</p>

<p>簡単な設定のメモ書きを残しておきます。</p>

<!-- more -->


<h2>HiveServer2について</h2>

<p>自分は以下の資料などを利用して理解を深めていった。</p>

<iframe src="//www.slideshare.net/slideshow/embed_code/28060369" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/schubertzhang/hiveserver2" title="HiveServer2" target="_blank">HiveServer2</a> </strong> from <strong><a href="http://www.slideshare.net/schubertzhang" target="_blank">Schubert Zhang</a></strong> </div></p>

<p>Clientについてはオフィシャルから。</p>

<p><a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients">HiveServer2 Clients - Apache Hive - Apache Software Foundation</a></p>

<h2>HiveServer2の起動</h2>

<p>CDH4をインストール済みであれば、HiveServer2の起動は以下のようにして行える。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ service hive-server2 start</span></code></pre></td></tr></table></div></figure>


<p>jpsで起動状態の確認を行う。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'># jps
</span><span class='line'>:
</span><span class='line'>:
</span><span class='line'>2901 RunJar　←HiveServer2のプロセス
</span><span class='line'>:</span></code></pre></td></tr></table></div></figure>


<h2>PythonからHiveServer2に接続（pyhs2）</h2>

<p>PythonからHiveServer2に接続して操作するためには、HiveSever2用クライアントドライバをインストールする必要がある。</p>

<p>オフィシャルドキュメント（ <a href="https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2#SettingUpHiveServer2-PythonClientDriver">https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2#SettingUpHiveServer2-PythonClientDriver</a> ）にそって、ここではpyhs2を利用することにする。</p>

<p>pyhs2のインストールはpipでOK。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ pip install pyhs2</span></code></pre></td></tr></table></div></figure>


<p>HiveServer2を起動した状態で以下の様なサンプルプログラム(thrift_sample.py)を動かしてみる。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/usr/bin/env python
</span><span class='line'># -*- coding:utf-8 -*-
</span><span class='line'>
</span><span class='line'>import pyhs2
</span><span class='line'>
</span><span class='line'>conn = pyhs2.connect(host='localhost',
</span><span class='line'>                  port=10000,
</span><span class='line'>                  authMechanism="PLAIN",
</span><span class='line'>                  user='root',
</span><span class='line'>                  password='test',
</span><span class='line'>                  database='default')
</span><span class='line'>cur = conn.cursor()
</span><span class='line'>cur.execute("show tables")
</span><span class='line'>for i in cur.fetch():
</span><span class='line'>  print i
</span><span class='line'>cur.close()
</span><span class='line'>conn.close()</span></code></pre></td></tr></table></div></figure>


<p>しかし、今の状態だと以下の様なエラーが出ると思われる。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ python thrift_sample.py
</span><span class='line'>Traceback (most recent call last):
</span><span class='line'>  File "thrift_sample.py", line 27, in &lt;module&gt;
</span><span class='line'>    database='default')
</span><span class='line'>  File "/usr/local/lib/python2.7/dist-packages/pyhs2/__init__.py", line 6, in connect
</span><span class='line'>    from .connections import Connection
</span><span class='line'>  File "/usr/local/lib/python2.7/dist-packages/pyhs2/connections.py", line 6, in &lt;module&gt;
</span><span class='line'>    import sasl
</span><span class='line'>ImportError: No module named sasl</span></code></pre></td></tr></table></div></figure>


<p>pyhs2の依存パッケージ（sasl）をインストールする必要がある。そのままsaslをpipでインストールしてみると以下のようなエラーが出るはず。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ pip install sasl
</span><span class='line'>:
</span><span class='line'>:
</span><span class='line'>gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -Isasl -I/usr/include/python2.7 -c sasl/saslwrapper.cpp -o build/temp.linux-x86_64-2.7/sasl/saslwrapper.o
</span><span class='line'>
</span><span class='line'>cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++ [enabled by default]
</span><span class='line'>
</span><span class='line'>sasl/saslwrapper.cpp:21:23: fatal error: sasl/sasl.h: No such file or directory
</span><span class='line'>
</span><span class='line'>compilation terminated.
</span><span class='line'>
</span><span class='line'>error: command 'gcc' failed with exit status 1
</span><span class='line'>:</span></code></pre></td></tr></table></div></figure>


<p>libsasl2-devをインストールする必要がある。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo apt-get install libsasl2-dev</span></code></pre></td></tr></table></div></figure>


<p>もう一度実行⇒OK</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ pip install sasl</span></code></pre></td></tr></table></div></figure>


<p>これでもう一度上記のサンプルプログラム（thrift_sample.py）を実行すればうまく動く（はず）。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PythonからMongoDBを操作する]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/27/pymongo/"/>
    <updated>2014-08-27T13:14:26+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/27/pymongo</id>
    <content type="html"><![CDATA[<p><a href="http://yoshihitoaso.github.io/blog/2014/08/20/ubuntu-mongo/">以前</a>、Ubuntu上にMongoDBをインストールしてみました。</p>

<p>今回は、MongoDBをPythonから操作する方法についてまとめておくことにします。</p>

<!-- more -->


<h2>PyMongoのインストール方法</h2>

<p>pythonから利用するためのモジュールとしてPyMongoを利用します。</p>

<p>インストール方法は簡単で、pipであれば以下でOK。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ pip install pymongo</span></code></pre></td></tr></table></div></figure>


<h2>サンプル</h2>

<p>pymongoを利用したMongoDB接続のサンプル（localhost接続の例）</p>

<p>①検索（find）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/usr/bin/env python
</span><span class='line'># -*- coding:utf-8 -*-
</span><span class='line'>
</span><span class='line'>from pymongo import Connection
</span><span class='line'>
</span><span class='line'>#コネクション作成
</span><span class='line'>con = Connection('localhost', 27017)
</span><span class='line'>
</span><span class='line'>#コネクションからsampleデータベースを取得
</span><span class='line'>db = con.sample
</span><span class='line'>
</span><span class='line'>#sampleデータベースからfooコレクションを取得
</span><span class='line'>col = db.foo
</span><span class='line'>
</span><span class='line'>print "========find_one========"
</span><span class='line'>print col.find_one()
</span><span class='line'>
</span><span class='line'>print "========find========"
</span><span class='line'>for data in col.find():
</span><span class='line'>    print data
</span><span class='line'>
</span><span class='line'>print "========find_query========"
</span><span class='line'>for data in col.find({u'a':10}):
</span><span class='line'>    print data</span></code></pre></td></tr></table></div></figure>


<p>②データの登録（insert）</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/usr/bin/env python
</span><span class='line'># -*- coding:utf-8 -*-
</span><span class='line'>
</span><span class='line'>from pymongo import Connection
</span><span class='line'>
</span><span class='line'>#コネクション作成
</span><span class='line'>con = Connection('localhost', 27017)
</span><span class='line'>
</span><span class='line'>#コネクションからsampleデータベースを取得
</span><span class='line'>db = con.sample
</span><span class='line'>
</span><span class='line'>#sampleデータベースからfooコレクションを取得
</span><span class='line'>col = db.foo
</span><span class='line'>
</span><span class='line'>#データの更新
</span><span class='line'>col.insert({'b' : 10})
</span><span class='line'>
</span><span class='line'>for data in col.find({'b':10}):
</span><span class='line'>    print data</span></code></pre></td></tr></table></div></figure>


<p>※ちなみにinsertは何度でも実行可能。その場合、レコード単位に新たなObjectIdが付与される。</p>

<p>③更新</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/usr/bin/env python
</span><span class='line'># -*- coding:utf-8 -*-
</span><span class='line'>
</span><span class='line'>from pymongo import Connection
</span><span class='line'>
</span><span class='line'>#コネクション作成
</span><span class='line'>con = Connection('localhost', 27017)
</span><span class='line'>
</span><span class='line'>#コネクションからsampleデータベースを取得
</span><span class='line'>db = con.sample
</span><span class='line'>
</span><span class='line'>#sampleデータベースからfooコレクションを取得
</span><span class='line'>col = db.foo
</span><span class='line'>
</span><span class='line'>data = col.find_one({'b':10})
</span><span class='line'>data['b'] = 11
</span><span class='line'>#データの更新
</span><span class='line'>col.save(data)
</span><span class='line'>
</span><span class='line'>for data in col.find({u'b':11}):
</span><span class='line'>    print data
</span><span class='line'>
</span><span class='line'>con.disconnect()</span></code></pre></td></tr></table></div></figure>


<p>④データの削除(remove)</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/usr/bin/env python
</span><span class='line'># -*- coding:utf-8 -*-
</span><span class='line'>
</span><span class='line'>from pymongo import Connection
</span><span class='line'>
</span><span class='line'>#コネクション作成
</span><span class='line'>con = Connection('localhost', 27017)
</span><span class='line'>
</span><span class='line'>#コネクションからsampleデータベースを取得
</span><span class='line'>db = con.sample
</span><span class='line'>
</span><span class='line'>#sampleデータベースからfooコレクションを取得
</span><span class='line'>col = db.foo
</span><span class='line'>
</span><span class='line'>print "===before==="
</span><span class='line'>for data in col.find():
</span><span class='line'>    print data
</span><span class='line'>
</span><span class='line'>col.remove({'b':11})
</span><span class='line'>
</span><span class='line'>print "===after==="
</span><span class='line'>for data in col.find():
</span><span class='line'>    print data
</span><span class='line'>
</span><span class='line'>con.disconnect()</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Excelシートの書式をマクロですべて削除する]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/25/clear-excel-formats/"/>
    <updated>2014-08-25T22:16:06+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/25/clear-excel-formats</id>
    <content type="html"><![CDATA[<p>仕事柄Excelを使うことが多いです。</p>

<p>最近も回ってきた資料を更新していたら、列をコピーして挿入などすると、</p>

<p>「セルの書式が多すぎるため、書式を追加できません」</p>

<p>のエラーが出まくって暫く白目をむいたりした。</p>

<p>そんな時のメモ。</p>

<!-- more -->


<p>ネットで調べたところ、原因としては新しい書式設定をするたびにExcelがそのセルの書式パターンをスタイルとして過去分も蓄積するためらしい。</p>

<p>これが例えばExcel2007であれば、64000パターンを超えると上記のエラーが出るようだ。</p>

<p>Excelのホームタブで「スタイル」のところのプルダウンに大量に書式が出てきたら、このエラーが出やすくなっている資料なので、使い回す際は気をつけろということになる。</p>

<p>過去分の書式はマクロを利用して消す事ができる（Excelの標準機能は画面から一つずつ消していくしかない）。</p>

<p>削除マクロの例は以下の記事にありました。ありがとうございます。</p>

<p><a href="http://www.ixam.net/ms-excel/vba-bian/delete-style">書式（スタイル）と名前の定義をすべて削除する - ixam</a></p>

<p>シートの一番下のタブを右クリックして、「コードの表示」を選択して、マクロのソースに以下のコードを貼り付けて、実行すると過去の書式が消える。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Sub delete_name_and_style()
</span><span class='line'>
</span><span class='line'>    On Error Resume Next
</span><span class='line'>
</span><span class='line'>'名前定義を全削除（名前を関数その他に有効活用している場合はここは削除）
</span><span class='line'>
</span><span class='line'>    Dim N As Name
</span><span class='line'>    For Each N In ActiveWorkbook.Names
</span><span class='line'>        N.Delete
</span><span class='line'>    Next
</span><span class='line'>
</span><span class='line'>'書式（スタイル）定義を全削除
</span><span class='line'>
</span><span class='line'>    Dim M()
</span><span class='line'>
</span><span class='line'>    J = ActiveWorkbook.Styles.Count
</span><span class='line'>    ReDim M(J)
</span><span class='line'>    For i = 1 To J
</span><span class='line'>        M(i) = ActiveWorkbook.Styles(i).Name
</span><span class='line'>    Next
</span><span class='line'>    For i = 1 To J
</span><span class='line'>        If InStr("Hyperlink,Normal,Followed Hyperlink", _
</span><span class='line'>                    M(i)) = 0 Then
</span><span class='line'>            ActiveWorkbook.Styles(M(i)).Delete
</span><span class='line'>        End If
</span><span class='line'>    Next
</span><span class='line'>
</span><span class='line'>End Sub</span></code></pre></td></tr></table></div></figure>


<p>このコードは名前定義も削除するコードなので、必要無ければ該当部分は消しても良い。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[pyqueryでWebスクレイピング]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/25/pyquery/"/>
    <updated>2014-08-25T20:15:05+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/25/pyquery</id>
    <content type="html"><![CDATA[<p>pyqueryを使ってスクレイピングしてみた時のメモ。以下のサイトを参考にした。</p>

<p>1) <a href="http://tnakamura.hatenablog.com/entry/20110602/python_scraping_pyquery">PyQuery で再びスクレイピング入門 - present</a></p>

<p>2) GitHub: <a href="https://github.com/gawel/pyquery/">gawel/pyquery</a></p>

<!-- more -->


<h2>インストール</h2>

<p>pipでインストール</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>pip install pyquery</span></code></pre></td></tr></table></div></figure>


<p>でOK。pyqueryが依存しているlxmlもあわせてインストールされる。</p>

<h2>使い方</h2>

<p>使い方は簡単で、jquery的に、</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>from pyquery import PyQuery
</span><span class='line'>q = PyQuery(url='http://b.hatena.ne.jp/')
</span><span class='line'>
</span><span class='line'>for elem in q.find('a.entry-link'):
</span><span class='line'>#PyQuery
</span><span class='line'>    q2 = PyQuery(elem)
</span><span class='line'>    print q2.text()
</span><span class='line'>    print q2.attr('href')
</span><span class='line'>
</span><span class='line'>#lxml
</span><span class='line'>#    print elem.text
</span><span class='line'>#    print elem.get('href')</span></code></pre></td></tr></table></div></figure>


<p>こんな感じで使える。</p>

<p>短いけど以上。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LL DIVER 参加メモ]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/25/ll-diver-2014/"/>
    <updated>2014-08-25T19:52:58+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/25/ll-diver-2014</id>
    <content type="html"><![CDATA[<p>2014/08/23 にお台場の日本科学未来館で開催された 『<a href="http://ll.jus.or.jp/2014/">LL DIVER</a>』　に参加して来た。こちらはその時のメモ。</p>

<p>※資料がまだ全て公開されていないようだったので、資料が公開され次第、随時更新していくつもり。</p>

<p><img src="https://s3-ap-northeast-1.amazonaws.com/y-asoh.aws-demo.net/lldiver.logo_.jpg" title="lldiver" alt="lldiver" /></p>

<!-- more -->




<iframe src="https://www.flickr.com/photos/koyhoge/15010405316/in/set-72157646842871822/player/" width="640" height="428" frameborder="0" allowfullscreen webkitallowfullscreen mozallowfullscreen oallowfullscreen msallowfullscreen></iframe>


<p>※かなり自分の言葉に置き直している箇所があるので、解釈としておかしな部分もあるかもしれません。予めご了承ください。</p>

<h2>タイムテーブル</h2>

<p>当日のタイムテーブルは<a href="http://ll.jus.or.jp/2014/timetable">こちら</a>から確認出来る。</p>

<p>自分が聞いたのは以下のセッションです。</p>

<ul>
<li>○○ as Code</li>
<li>HerokuでGauche</li>
<li>Guraプログラミング言語の紹介</li>
<li>PythonによるWebスクレイピング入門</li>
<li>エディタ対決(仮)</li>
<li>LT</li>
</ul>


<h2>○○ as Code</h2>

<p>パネリストは以下の4名でした。</p>

<ul>
<li>高野祥幸さん</li>
<li>前佛雅人さん @zembutsu</li>
<li>吉田賢造さん</li>
<li>司会：佐々木健さん @sasakipochi</li>
</ul>


<h4>インフラのCode化：高野さん（ニフティクラウド）</h4>

<p>まずニフティクラウドの高野さんからインフラのCode化についての話がありました。</p>

<p>例えば、ニフティクラウドではインフラをコード（サンプルはJavaだった）から制御出来るようになっている。</p>

<ul>
<li>インフラ as Code：　インスタンス起動など</li>
<li>ミドルウェア as Code：　DBをたてる（バックアップの設定なども可能）など</li>
<li>バックエンド as Code：　よりインフラを隠蔽化されていたり</li>
</ul>


<p>続いて、システム開発における各工程（要件定義、設計、開発、テスト、リリース）をCode化についての話がありました。</p>

<ul>
<li>各工程はコード化されている</li>
<li>設計：今だとテスト駆動（TDD）でやったりする</li>
<li>開発：色々な言語で開発する</li>
<li>テスト：SeleniumとかXUnitとか、コードを書いてテストする</li>
<li>リリース：AnsibleやChefでコード化出来る</li>
<li>唯一要件定義工程がコード化されていない。</li>
<li>コード化出来ないならなくせば？という発想で今考え始めているところ。答えはまだ無い。</li>
</ul>


<h4>農業のCode化：前佛さん</h4>

<p>農業のシステム化が語られることが多くなったが、提供されている農業ソリューションが高価すぎるという問題がある。</p>

<p>農業分野にもOSSやクラウドコンピューティングの考えが導入出来ないか？と考えている。</p>

<p>運用監視の自動化の話。</p>

<ul>
<li>昔に比べてサービスが落ちた時の影響度が甚大である</li>
<li>全て人手では運用が回らない状態。運用の自動化・Code化が必要。</li>
<li>運用監視としてMuninやZabix、オーケストレーションツール（Serf、Consul）などで自動化したり。</li>
<li>運用のコード化の流れ

<ul>
<li>作業手順書作成</li>
<li>監視ツール導入</li>
<li>自動化技術の導入</li>
</ul>
</li>
</ul>


<p>農業への適用を考えたいという話。</p>

<ul>
<li>農具で言うと、牛→餌の食わない牛（機械）へと変化していった</li>
<li>なぜ農業で自動化・コード化が必要？

<ul>
<li>人が足りてない</li>
<li>商品での差別化は難しい（例えば稲作とか）</li>
</ul>
</li>
<li>重要になってくる分野

<ul>
<li>情報化、自動化、ロボット技術</li>
<li>流れの中でコミュニケーション用のツールとしてslack, hubotを利用してみたら良いかもみたいな話があった。</li>
</ul>
</li>
</ul>


<h4>工業のCode化：吉田さん（株式会社スマメ代表など）</h4>

<p>3Dプリンタ関連の導入支援を行っている（DMM.makeとか）。その他にしぶや図工室、Fabbitなど。</p>

<p>3Dプリンタ業界のぶっちゃけ話</p>

<ul>
<li>プレーヤーがずっと変わってない（市場的に成熟してしまっている）</li>
<li>金額が色々とおかしい（日本では価格が高すぎる）</li>
<li>そもそも思っている程大したものではない

<ul>
<li>出来る形状と出来ない形状がある</li>
<li>独自の素材しか使えない</li>
<li>材料費が高い</li>
<li>データ形式（stl）が汎用的ではない（かなり前から存在する形式）</li>
</ul>
</li>
</ul>


<p>3Dプリンタが使えると思われる分野</p>

<ul>
<li>補聴器</li>
<li>義歯、義足、人工骨/インプラント</li>
</ul>


<h4>ディスカッション</h4>

<p>Googleカーみたいなものは農業分野で役立つか？みたいな話があった。</p>

<p>これについては、まだやや飛躍気味で、機械学習が出来るような知見がたまるまでに少し時間がかかるだろうという話であった。</p>

<p>それよりももっと細かいところでの改善がまだまだ必要で、田んぼに引き込む水量の制御など、自動化出来るところがまだまだあるのでは？といった話がされていた。</p>

<p>農業分野に3Dプリンタが生かせないか？みたいな話があった。</p>

<ul>
<li>壊れても良い、安価な機械を３Dプリンタで作ることが出来たらおもしろいかも</li>
<li>情報を公開することで、ノウハウの共有、オープンソース化が出来る。</li>
</ul>


<h2>HerokuでGauche (あるいは、好きな言語何でも)：久井亨さん</h2>

<p>資料のありか：<a href="http://torus.github.io/raytracer/ll2014/#slide1">Heroku で Gauche（あるいは好きな言語なんでも）</a></p>

<p>Herokuは色々な言語が使える。ただし、GaucheのようなScheme処理系はサポートされていない。</p>

<p>Herokuの内部構造</p>

<ul>
<li>App

<ul>
<li>Cedar Stack　（ベースOS Ubuntu 10.4）

<ul>
<li>Dyno</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>HerokuでScheme処理系言語を使いたいなら、Cedar Stack上でSlugを利用すればOK。</p>

<p>参考：<a href="http://torus.tumblr.com/post/87300539228/heroku-gauche">Heroku で Gauche のアプリケーションを動かす</a></p>

<p>ただし、Ubuntu10.04でコンパイルする必要がある。Dockerを使うと便利。</p>

<h2>PythonによるWebスクレイピング入門：関根裕紀さん</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/38279221" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/checkpoint77/pythonweb-38279221" title="PythonによるWebスクレイピング入門" target="_blank">PythonによるWebスクレイピング入門</a> </strong> from <strong><a href="http://www.slideshare.net/checkpoint77" target="_blank">checkpoint77</a></strong> </div></p>

<p>Webスクレイピングの方法</p>

<ul>
<li>1)Webサービスを使う（Yahoo! Pipesとか）</li>
<li>2)プログラム言語を使う</li>
</ul>


<p>pythonの場合は以下の選択肢がある</p>

<ul>
<li>標準ライブラリ　urllib2</li>
<li>beautiful soup</li>
<li>pyquery</li>
<li>scrapy</li>
</ul>


<p>それぞれ特徴があり、できる事も違う。用途によって使い分ける感じになると思う。</p>

<h2>その他のスライドのありか</h2>

<h3>GMOペパボのエンジニア新人研修</h3>

<p><a href="http://blog.kentarok.org/entry/2014/08/23/145417">GMOペパボのエンジニア新人研修 #lldiver - delirious thoughts</a></p>

<script async class="speakerdeck-embed" data-id="d0e869100cb601321e855e226fb19d76" data-ratio="1.33333333333333" src="//speakerdeck.com/assets/embed.js"></script>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[UbuntuにMongoDBをインストールする]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/20/ubuntu-mongo/"/>
    <updated>2014-08-20T15:54:27+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/20/ubuntu-mongo</id>
    <content type="html"><![CDATA[<p>UbuntuにMongoDBをインストールしたときのメモ。</p>

<p>※参考情報：OSのバージョン</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat /etc/lsb-release
</span><span class='line'>DISTRIB_ID=Ubuntu
</span><span class='line'>DISTRIB_RELEASE=12.10
</span><span class='line'>DISTRIB_CODENAME=quantal
</span><span class='line'>DISTRIB_DESCRIPTION="Ubuntu 12.10"</span></code></pre></td></tr></table></div></figure>


<h2>MongoDBをインストール</h2>

<p>公式ドキュメントの通り、以下の手順でインストール出来る</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10
</span><span class='line'>$ echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | sudo tee /etc/apt/sources.list.d/mongodb.list
</span><span class='line'>$ sudo apt-get update
</span><span class='line'>$ sudo apt-get install mongodb-10gen</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>MongoDBの起動</h2>

<p>起動停止は以下のコマンドで実施する。</p>

<p>MongoDBのプロセスを開始</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo service mongodb start</span></code></pre></td></tr></table></div></figure>


<p>MongoDBのプロセスを停止</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo service mongodb stop</span></code></pre></td></tr></table></div></figure>


<p>MongoDBのプロセスを再起動</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo service mongodb restart</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu上にCDH4（Hadoop）擬似分散モード環境を構築する （※おまけでHiveをインストール）]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/20/cdh4-pseudo-distributed-mode/"/>
    <updated>2014-08-20T15:44:13+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/20/cdh4-pseudo-distributed-mode</id>
    <content type="html"><![CDATA[<p>Ubuntuサーバ上にCDH4を擬似分散モードでインストールした時のメモ。</p>

<p>※参考情報：OSのバージョン</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat /etc/lsb-release
</span><span class='line'>DISTRIB_ID=Ubuntu
</span><span class='line'>DISTRIB_RELEASE=12.10
</span><span class='line'>DISTRIB_CODENAME=quantal
</span><span class='line'>DISTRIB_DESCRIPTION="Ubuntu 12.10"</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>Oracle-Javaのインストール</h2>

<p>Java7をインストールする。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo add-apt-repository ppa:webupd8team/java
</span><span class='line'>$ sudo apt-get update
</span><span class='line'>$ sudo apt-get install oracle-java7-installer</span></code></pre></td></tr></table></div></figure>


<p>バージョンの確認</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ java -version
</span><span class='line'>java version "1.7.0_51"
</span><span class='line'>Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
</span><span class='line'>Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)
</span><span class='line'>$ javac -version
</span><span class='line'>javac 1.7.0_51</span></code></pre></td></tr></table></div></figure>


<p>pathにJAVA_HOMEを設定しておく必要がある。
.bashrcに以下の設定を追記。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export JAVA_HOME=/usr/lib/jvm/java-7-oracle
</span><span class='line'>export PATH=$PATH:$JAVA_HOME/bin</span></code></pre></td></tr></table></div></figure>


<h2>CDH4のインストール</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget http://archive.cloudera.com/cdh4/one-click-install/precise/amd64/cdh4-repository_1.0_all.deb
</span><span class='line'>$ sudo dpkg -i cdh4-repository_1.0_all.deb
</span><span class='line'>$ curl -s http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh/archive.key | sudo apt-key add -
</span><span class='line'>
</span><span class='line'>$ sudo apt-get update
</span><span class='line'>$ sudo apt-get install hadoop-conf-pseudo</span></code></pre></td></tr></table></div></figure>


<p>ディレクトリを確認。各種設定ファイルが格納されている。
通常起動の場合は編集の必要はない。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ls -l /etc/hadoop/conf.pseudo
</span><span class='line'>total 40
</span><span class='line'>-rw-r--r-- 1 root hadoop 1458 Feb 26 09:54 core-site.xml
</span><span class='line'>-rw-r--r-- 1 root hadoop 1364 Feb 26 09:54 hadoop-env.sh
</span><span class='line'>-rw-r--r-- 1 root hadoop 2890 Feb 26 09:54 hadoop-metrics.properties
</span><span class='line'>-rw-r--r-- 1 root hadoop 1875 Feb 26 09:54 hdfs-site.xml
</span><span class='line'>-rw-r--r-- 1 root hadoop 8735 Feb 26 10:21 log4j.properties
</span><span class='line'>-rw-r--r-- 1 root hadoop 1549 Feb 26 09:54 mapred-site.xml
</span><span class='line'>-rw-r--r-- 1 root hadoop 1104 Feb 26 09:54 README
</span><span class='line'>-rw-r--r-- 1 root hadoop 2361 Feb 26 09:54 yarn-site.xml</span></code></pre></td></tr></table></div></figure>


<h2>HDFS /tmp directory を作成する</h2>

<p>hdfsユーザでHDFSのフォーマットを実施する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo -u hdfs
</span><span class='line'>$ hdfs namenode -format</span></code></pre></td></tr></table></div></figure>


<p>HDFSを起動。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ /etc/init.d/hadoop-hdfs-namenode
</span><span class='line'>$ /etc/init.d/hadoop-hdfs-datanode
</span><span class='line'>$ /etc/init.d/hadoop-hdfs-secondarynamenode</span></code></pre></td></tr></table></div></figure>


<p>とすれば良いけど、毎回このコマンドを打つのは面倒なので以下の様なスクリプト（hadoop-hdfs-start）を作っておく。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>for service in /etc/init.d/hadoop-hdfs-*
</span><span class='line'>do
</span><span class='line'>sudo $service start
</span><span class='line'>done</span></code></pre></td></tr></table></div></figure>


<p>停止の場合も同様。以下の様なスクリプト（hadoop-hdfs-stop）を作っておくとラク。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>for service in /etc/init.d/hadoop-hdfs-*
</span><span class='line'>do
</span><span class='line'>sudo $service stop
</span><span class='line'>done</span></code></pre></td></tr></table></div></figure>


<p>起動を実行したら、各サービスが起動していることを確認する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo $JAVA_HOME/bin/jps
</span><span class='line'>4233 Jps
</span><span class='line'>2476 NodeManager
</span><span class='line'>2007 SecondaryNameNode
</span><span class='line'>2622 ResourceManager
</span><span class='line'>1827 NameNode
</span><span class='line'>1652 DataNode</span></code></pre></td></tr></table></div></figure>


<h2>必要なDirectoryを作成する</h2>

<p>HDFS上に必要なディレクトリを作成していく。この作業もhdfsユーザで実施する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo su - hdfs
</span><span class='line'>$ hadoop fs -mkdir /tmp　←HDFS上に/tmpを作成
</span><span class='line'>$ hadoop fs -chmod -R 1777 /tmp
</span><span class='line'>$ hadoop fs -mkdir /var/log/hadoop-yarn
</span><span class='line'>$ hadoop fs -chown yarn:mapred /var/log/hadoop-yarn
</span><span class='line'>$ hadoop fs -mkdir /tmp/hadoop-yarn/staging
</span><span class='line'>$ hadoop fs -chmod -R 1777 /tmp/hadoop-yarn/staging
</span><span class='line'>$ hadoop fs -mkdir /tmp/hadoop-yarn/staging/history/done_intermediate
</span><span class='line'>$ hadoop fs -chmod -R 1777 /tmp/hadoop-yarn/staging/history/done_intermediate
</span><span class='line'>$ hadoop fs -mkdir /user/$USER　←hdfsユーザのホームディレクトリ/user/hdfs
</span><span class='line'>$ hadoop fs -chown hdfs /user/$USER</span></code></pre></td></tr></table></div></figure>


<p>/user/hdfsが作業ディレクトリと考えてよい。上記実行中エラーが出て中断するようなら、HDFSが落ちている可能性があるので起動状態を確認してみる。</p>

<p>HDFS上のlsは以下のようにして実行する。上記で作ったtmp,user,varができてればOK。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ hadoop fs -ls /
</span><span class='line'>Found 3 items
</span><span class='line'>drwxrwxrwt – hdfs supergroup 0 2013-01-05 12:10 /tmp
</span><span class='line'>drwxr-xr-x – hdfs supergroup 0 2013-01-05 12:36 /user
</span><span class='line'>drwxr-xr-x – hdfs supergroup 0 2013-01-05 12:09 /var</span></code></pre></td></tr></table></div></figure>


<h2>Start YARN !!!!!</h2>

<p>管理ユーザにスイッチしてYARN (MapReduce)を起動する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo service hadoop-yarn-resourcemanager start
</span><span class='line'>$ sudo service hadoop-yarn-nodemanager start
</span><span class='line'>$ sudo service hadoop-mapreduce-historyserver start</span></code></pre></td></tr></table></div></figure>


<p>プロセスの確認。以下の様な感じになっていればOK。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo $JAVA_HOME/bin/jps
</span><span class='line'>2598 ResourceManager
</span><span class='line'>4798 JobHistoryServer
</span><span class='line'>4840 Jps
</span><span class='line'>1752 NameNode
</span><span class='line'>2448 NodeManager
</span><span class='line'>2043 SecondaryNameNode
</span><span class='line'>1496 DataNode</span></code></pre></td></tr></table></div></figure>


<h1>おまけ）Hiveをインストールして使ってみる</h1>

<h2>Hiveのインストール</h2>

<p>以下の手順でHiveをインストールする。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd Downloads/
</span><span class='line'>$ mkdir hive
</span><span class='line'>$ cd hive/
</span><span class='line'>$ wget http://mirror.tcpdiag.net/apache/hive/stable/hive-0.11.0.tar.gz
</span><span class='line'>$ tar xzf hive-0.11.0.tar.gz
</span><span class='line'>$ mkdir /usr/lib/hive
</span><span class='line'>$ mv hive-0.11.0 /usr/lib/hive/hive-0.11.0</span></code></pre></td></tr></table></div></figure>


<p>環境変数を設定する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd
</span><span class='line'>$ vim .bashrc
</span><span class='line'>$ export HIVE_HOME=/usr/lib/hive/hive-0.11.0
</span><span class='line'>$ export PATH=$PATH:$HIVE_HOME/bin</span></code></pre></td></tr></table></div></figure>


<h3>Login</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ hive
</span><span class='line'>...
</span><span class='line'>..
</span><span class='line'>.
</span><span class='line'>
</span><span class='line'>hive&gt;</span></code></pre></td></tr></table></div></figure>


<p>でOK。</p>

<h2>Sample : Create Table</h2>

<p>Tableを作成して、CSV形式のファイルをロードしてみる。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hive&gt; create table cats (id int, name string, birthday string) row format delimited fields terminated by ',' lines terminated by '\n';</span></code></pre></td></tr></table></div></figure>


<p>csv形式のファイルを以下のように用意する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ vim cats.csv
</span><span class='line'>
</span><span class='line'>1,Tama,19929423
</span><span class='line'>2,Tora,19930304
</span><span class='line'>3,Nya,19930304
</span></code></pre></td></tr></table></div></figure>


<p>以下のコマンドでロード出来る。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hive&gt; load data local inpath '/root/hadoop/hive/cats.csv' overwrite into table cats;</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[straceでjavaプロセスの挙動を確認した]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/11/strace/"/>
    <updated>2014-08-11T11:03:25+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/11/strace</id>
    <content type="html"><![CDATA[<h2>strace コマンドとは</h2>

<p>この辺が参考になる。</p>

<blockquote><p><a href="http://blog.livedoor.jp/sonots/archives/18193659.html">strace コマンドの使い方をまとめてみた - sonots:blog</a></p></blockquote>

<p>straceを利用して、プロセスが呼び出すシステムコールをトレースすることが出来る。</p>

<h2>コマンドからトレース</h2>

<p>実行コマンドが分かっている場合は、strace の後にトレースしたいコマンドを記述する感じ。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ strace [command]</span></code></pre></td></tr></table></div></figure>


<p>という感じで実行すれば良い。</p>

<h2>プロセスを指定</h2>

<p>実行中プロセスをトレースしたい場合がある。pid 指定でトレースする。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>strace -p [pid]</span></code></pre></td></tr></table></div></figure>


<h2>具体例</h2>

<p>例が渋すぎるが、最近実行した例を1つ。
JBossのtwiddleがある環境でロングランする事象があったので調査した。
切り分けし易いように、twiddle.shではなく、内部で実行しているjavaコマンドを直接実行してトレースした。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>strace -tt -f -s 1024 -o strace.log java -classpath /&lt;任意のpath&gt;/twiddle.jar:/&lt;任意のpath&gt;/jbossall-client.jar:/&lt;任意のpath&gt;/jboss-common.jar:/&lt;任意のpath&gt;/getopt.jar:/&lt;任意のpath&gt;/log4j.jar:/&lt;任意のpath&gt;/jboss-jmx.jar:/&lt;任意のpath&gt;/dom4j-1.5.jar org.jboss.console.twiddle.Twiddle -s &lt;任意のurlやip&gt;:&lt;任意のport&gt; &lt;command&gt; [ command_arguments</span></code></pre></td></tr></table></div></figure>


<p>オプションが幾つかついているが補足しておくと、</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-tt: マイクロ秒単位で表示
</span><span class='line'>-f: 内部でスレッドが切れている場合、スレッドまでトレースする場合
</span><span class='line'>-s: 1行あたり表示される文字数を変更する。（デフォルトは32文字）
</span><span class='line'>-o: ファイルに出力するオプション</span></code></pre></td></tr></table></div></figure>


<p>といった感じ。</p>

<p>-f オプションの場合、別プロセスのトレース結果が同一ファイルに出力される。別ファイルにしたい場合は、-ff オプションにすれば良い。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Conference Japan 2014　参加メモ #hcj2014]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/08/hadoop-conference-japan-2014-number-hcj2014/"/>
    <updated>2014-08-08T21:49:15+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/08/hadoop-conference-japan-2014-number-hcj2014</id>
    <content type="html"><![CDATA[<p><img src="https://s3-ap-northeast-1.amazonaws.com/y-asoh.aws-demo.net/logo.png" title="hadoop" alt="hadoop" /></p>

<p>メモは残してそのままにしていたが、こちらに書いておくことにする。</p>

<!-- more -->


<p>Hadoop Conference Japan 2014 に参加してきたので、その参加メモ。</p>

<p>7/8に開催された Hadoop Conference Japan 2014に参加してきた（去年に引き続き2回目の参加）。</p>

<p>そもそもHadoop Conference Japanとは、並列分散処理フレームワーク Apache Hadoop および周辺のオープンソースソフトウェアに関するユーザカンファレンス。
日本Hadoopユーザー会の有志によって運営されており、今回で5回目の開催になる。</p>

<p>↓各種レポート記事へのリンク↓</p>

<ul>
<li><strong>gihyo.jp</strong>

<ul>
<li><a href="http://gihyo.jp/news/report/2014/07/0902">日本よ，これが2014年のHadoopだ！─「Hadoop Conference Japan 2014」基調講演レポート</a></li>
</ul>
</li>
<li><strong>ITpro</strong>

<ul>
<li><a href="http://itpro.nikkeibp.co.jp/article/NEWS/20140708/569985/">「Hadoopはビッグデータの“OSカーネル”」、Hadoop Conference Japan開催</a></li>
<li><a href="http://itpro.nikkeibp.co.jp/article/COLUMN/20140711/570863/">NTTデータが4000コアのクラスターでSparkを試行、NTTドコモからの要望受け</a></li>
</ul>
</li>
<li><strong>Publickey</strong>

<ul>
<li><a href="http://www.publickey1.jp/blog/14/yarnhadoophadoop_conference_japan_2014.html">YARNの登場によりHadoopは複数の並列分散処理エンジンを併用できる環境へ</a></li>
<li><a href="http://www.publickey1.jp/blog/14/hadoopoltphadoop_conference_japan_2014.html">HadoopはいずれOLTPも実現し、エンタープライズデータハブとなる</a></li>
</ul>
</li>
</ul>


<h1>タイムテーブル</h1>

<p>当日のタイムテーブルは<a href="https://www.eventbrite.com/e/hadoop-conference-japan-2014-tickets-12016613013">こちら</a></p>

<p>午前のキーノートから始まり、昼食（無料！）を挟んで、午後はHadoopに関する技術トピックや活用事例を紹介する発表、という流れ。</p>

<p>以下は、自分が聞いた発表</p>

<p>【午前：キーノート】</p>

<ul>
<li>米谷 修 （リクルートテクノロジーズ）、濱野 賢一朗 （日本Hadoopユーザー会, NTTデータ）</li>
<li>Doug Cutting （Hadoop生みの親, Apache Software Foundation, Cloudera） 『The Future of Data』</li>
<li>Patrick Wendell （Apache Spark主要開発者, Databricks） 『The Future of Spark』 [講演資料]</li>
<li>太田 一樹 （Treasure Data CTO） 『Hadoopエコシステムの変遷と、見えてきた使いどころ』</li>
</ul>


<p>【午後】</p>

<ul>
<li>リクルート式Hadoopの使い方 3rd Edition : 石川 信行（リクルートテクノロジーズ）</li>
<li>SQLによるバッチ処理とストリーム処理 : 田籠 聡 (LINE)</li>
<li>A Deeper Understanding of Spark Internals : Patrick Wendell （Databricks）</li>
<li>Spark1.0での動作検証 - Hadoopユーザ・デベロッパから見たSparkへの期待 : 土橋 昌 （NTTデータ）</li>
<li>Treasure Data on The YARN : 小林 隆（Treasure Data）</li>
<li>並列SQLエンジンPresto - 大規模データセットを高速にグラフ化する方法 : 古橋 貞之（Treasure Data）</li>
</ul>


<h1>午前：キーノート</h1>

<h2>濱野 賢一朗 （日本Hadoopユーザー会, NTTデータ）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36781484" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/hamaken/hadoop-conference-japan-2014-36781484" title="Hadoop Conference Japan 2014 ご挨拶・Hadoopを取り巻く環境" target="_blank">Hadoop Conference Japan 2014 ご挨拶・Hadoopを取り巻く環境</a> </strong> from <strong><a href="http://www.slideshare.net/hamaken" target="_blank">hamaken</a></strong> </div></p>

<ul>
<li>今回の参加者数は1296名。そのうち約65%が初参加とのこと。</li>
<li>Hadoopは絶賛進化中。現在は2.X系の開発が中心</li>
<li>その中心的なトピックは「YARN」（※）</li>
<li>日本におけるHadoopの利用ユーザは徐々に増えており、今回の参加者の44%以上が6ヶ月以上の利用経験を有する</li>
<li>利用しているエコシステムとしてはHive、HBaseが多い。Impala、Sparkなどの比較的新しいプロダクトについても利用ユーザは相当数いる。</li>
</ul>


<blockquote><p><strong>YARN（Yet Another Resource Negotiator）についての参考資料</strong></p>

<ul>
<li>Apache Hadoop 2.4.1 - YARN <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html</a></li>
<li>YARN の紹介 <a href="http://www.ibm.com/developerworks/jp/bigdata/library/bd-yarn-intro/">http://www.ibm.com/developerworks/jp/bigdata/library/bd-yarn-intro/</a></li>
</ul>
</blockquote>

<h2>Doug Cutting （Hadoop生みの親、Apache Software Foundation, Cloudera）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36826369" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/Cloudera_jp/the-future-of-data-jp" title="The future of data by Doug Cutting #hcj2014" target="_blank">The future of data by Doug Cutting #hcj2014</a> </strong> from <strong><a href="http://www.slideshare.net/Cloudera_jp" target="_blank">Cloudera Japan</a></strong> </div></p>

<ul>
<li>ハードウェアの価格は更に安く、データの価値は更に高まる（よりハイレゾリューションに）、そのためのソフトウェアが必要になる</li>
<li>そしてオープンソースが勝ち残る (luceneは 1999 から開発開始しているが、事実生き残っている）</li>
<li>オープンソースは使う側から見てもリスクを低減する。プラットフォーム技術については特にOSSであることが必要とされる。</li>
<li>Hadoopの機能はさらに向上し、Hadoopが当たり前になるだろう</li>
<li>GoogleのSpannerのようにトランザクション処理をHadoop上でサポートするという話があった（※）</li>
</ul>


<blockquote><p><strong>※参考記事</strong></p>

<ul>
<li>Hadoop creator: &lsquo;Google is living a few years in the future and sending the rest of us messages&rsquo; | ZDNet <a href="http://www.zdnet.com/hadoop-creator-google-is-living-a-few-years-in-the-future-and-sending-the-rest-of-us-messages-7000023160/">http://www.zdnet.com/hadoop-creator-google-is-living-a-few-years-in-the-future-and-sending-the-rest-of-us-messages-7000023160/</a></li>
</ul>
</blockquote>

<h2>Patrick Wendell （Apache Spark主要開発者、 Databricks）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36856077" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/hadoopconf/the-future-of-apache-spark" title="The Future of Apache Spark" target="_blank">The Future of Apache Spark</a> </strong> from <strong><a href="http://www.slideshare.net/hadoopconf" target="_blank">Hadoop Conference Japan</a></strong> </div></p>

<ul>
<li>Spark開発者の一週間の活動

<ul>
<li>パッチの投稿や修正等：500件、JIRA/GitHub上でのコメント：200件、メール：140スレッド、マージされるパッチの数：80件</li>
</ul>
</li>
<li>Sparkはここ1年で急速に成長した</li>
<li>APIの安定性を重視している。APIを破壊するようなパッチはビルドに失敗するようにしている</li>
<li>開発者にやさしいリリースサイクルでリリースを行っている。マイナーリリースは3ヶ月毎、必要に応じてメンテナンスリリースを行っている。</li>
<li>Sparkの主要エコシステムはSpark SQL（SQL on Spark）、MLLib（機械学習ライブラリ）、GraphX（グラフDB）、Spark Streaming（ストリーム処理）</li>
<li>そのうちSpark SQLは他のコンポーネントよりも早く成長している。MLLibがその次。</li>
<li>SparkR といプロダクトもある。RでSparkを動かす。</li>
<li>Databricks Cloudのデモ。Web UI上でSparkのプログラムが書ける。SQLも実行できる。その実行結果を簡単にグラフ表示ができる。すごい。</li>
</ul>


<blockquote><p><strong>Apache Spark に関する参考資料</strong></p>

<ul>
<li>Apache Spark の紹介（前半：Sparkのキホン） <a href="http://www.slideshare.net/hadoopxnttdata/apache-spark-spark">http://www.slideshare.net/hadoopxnttdata/apache-spark-spark</a></li>
<li>Apache Sparkのご紹介 （後半：技術トピック） <a href="http://www.slideshare.net/hadoopxnttdata/apache-spark">http://www.slideshare.net/hadoopxnttdata/apache-spark</a></li>
<li>Spark: 高速なデータ分析のための新たな手段 <a href="http://www.ibm.com/developerworks/jp/opensource/library/os-spark/">http://www.ibm.com/developerworks/jp/opensource/library/os-spark/</a></li>
</ul>
</blockquote>

<h2>太田 一樹 （Treasure Data CTO）</h2>

<p><strong>『Hadoopエコシステムの変遷と、見えてきた使いどころ』（資料未公開）</strong></p>

<ul>
<li>ただ単に「安いストレージ的」に使うのであれば、GlusterFSやCephといったストレージ特化の様々なプロダクトがある。こちらのほうが優れてる。</li>
<li><p><strong>Hadoop エコシステムの進化と混沌</strong> 4つの側面で様々なプロダクトが乱立している</p>

<ul>
<li>1.Collect Any Types of data

<ul>
<li>様々なデータ収集ミドルウェアの開発が進んでいる</li>
<li>Fluentd, Kafka, Flume, Sqoop</li>
</ul>
</li>
<li>2.Store Any types of data economically

<ul>
<li>Parquet, ORCFile (format)</li>
<li>HDFS, HBase, Accumulo</li>
<li>Ambali, HUE, Cluudera Maager 管理/運用の支援は大事</li>
<li>Treasure Data, AWS EMR</li>
</ul>
</li>
<li>3.Faster Use of Data

<ul>
<li>いかに早くデータを扱うか？</li>
<li>YARN</li>
<li>Storm, Samza, Norikra</li>
<li>Apache Tez, Spark</li>
<li>HiveQL, Pig</li>
<li>Java: Cascading, Apache Crunch</li>
</ul>
</li>
<li>4.Better Use of Data

<ul>
<li>いかにうまくデータを扱うか？</li>
<li>SQL on Hadoop：Impala, SparkSQL, Presto, Drill</li>
<li>機械学習：Mahout, Spark MLlib, Hivemall</li>
</ul>
</li>
</ul>
</li>
<li><p>他の選択肢の進化</p>

<ul>
<li>  Database の進化

<ul>
<li>  MPP（Massiely Parallel Processing） MPPデータベース（RedShiftやBigQueryなど）　（※）</li>
<li>  Schema-on-Write （先にスキーマを決めないといけない） 。アドホックな解析をするためのデータベースとしては少し不向き。</li>
<li>  Oracle, DB2, SQLserver</li>
<li>  Teradata, Netezza, Vertica, ParAccel, Greenplum</li>
<li>  多くのベンダーが Hadoop 対応を表明</li>
<li>  基本的にはスキーマを決めないといけないが、Vertica Zonemap など Schema-on-Read 対応なども</li>
<li>  現在の主流

<ul>
<li>  Hadoop に生データをすべて集約</li>
<li>  そこから集計集約したデータを MPP データベースに保存</li>
</ul>
</li>
<li>  Hadoop は構造化データとの境界線に</li>
<li>  MPP データベースは非構造化データの領域に踏み込む</li>
<li>  誰がマーケットリーダーになっていくのか注視していかないといけない</li>
<li>  使用する側は一層の知識とトレンドの把握が必要。</li>
</ul>
</li>
</ul>
</li>
</ul>


<blockquote><p><strong>※参考資料</strong></p>

<p>MPPの使い分けの話</p>

<ul>
<li>MPP on Hadoop, Redshift, BigQuery - Go ahead! <a href="http://repeatedly.github.io/ja/2014/07/mpp-on-hadoop-redshift-bigquery/">http://repeatedly.github.io/ja/2014/07/mpp-on-hadoop-redshift-bigquery/</a></li>
</ul>
</blockquote>

<h1>午後</h1>

<h2>リクルート式Hadoopの使い方 3rd Edition : 石川 信行（リクルートテクノロジーズ）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36770927" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/recruitcojp/hadoop20140707" title="Hadoopカンファレンス20140707" target="_blank">Hadoopカンファレンス20140707</a> </strong> from <strong><a href="http://www.slideshare.net/recruitcojp" target="_blank">Recruit Technologies</a></strong> </div></p>

<ul>
<li>リクルートグループのHadoop活用事例に関する発表</li>
<li>様々な利用用途でHadoop、およびHadoopのエコシステムを利用している。詳細は資料の通り。</li>
<li>リクルートでは2010年からHadoopの研究開発、2011年から本格展開を始めた</li>
<li>2014年はアドホック分析基盤の導入等の活動を行っている</li>
<li>導入に際しては、既存サービスを生かしつつ、徐々に利用範囲を広げていっている（安く、早く提供）</li>
<li>本番提供しているサービスの他にも、現在も様々な技術検証を並行して実施している。

<ul>
<li>画像解析（スパースコーディングなど）</li>
<li>テキスト解析（Skip-Gramなど）</li>
<li>グラフ（Titanなど）</li>
</ul>
</li>
</ul>


<h2>並列SQLエンジンPresto - 大規模データセットを高速にグラフ化する方法 : 古橋 貞之（Treasure Data）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36771514" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/frsyuki/presto-hadoop-conference-japan-2014" title="Presto - Hadoop Conference Japan 2014" target="_blank">Presto - Hadoop Conference Japan 2014</a> </strong> from <strong><a href="http://www.slideshare.net/frsyuki" target="_blank">Sadayuki Furuhashi</a></strong> </div></p>

<ul>
<li><p>1.イントロ</p>

<ul>
<li>PrestoとはGBからPBのデータ分析を対話的に行う分散SQLクエリエンジン</li>
<li>2012秋からFacebookで開発された</li>
<li>Prestoが解決しようとする問題は

<ul>
<li>BIツールから直接HDFS上のデータを可視化できない</li>
<li>日次バッチでPostgreSQLやRedshiftにデータを入れる必要がある</li>
<li>いくつかのデータはHDFS上にはない。そのため分析時にHDFS上にコピーが必要</li>
</ul>
</li>
<li>Prestoだとクエリをミリ秒から分で処理。ただしETLにはMapReduceやHiveは必要</li>
<li>BIツールから接続可能。ODBC/JDBCコネクタ</li>
<li>複数のデータソースにまたがってクエリを実行可能</li>
</ul>
</li>
<li><p>2.分散アーキテクチャ</p>

<ul>
<li>Discovery Service</li>
<li>Coordinator</li>
<li>Worker</li>
<li><p>Connector Plugin : ConnectorはJavaで書かれており、ストレージとメタデータの実装。自作も可能。</p>

<ul>
<li>Hive Connector</li>
<li>Cassandra Connector</li>
<li>MySQL through JDBC Connector(prerelease)</li>
</ul>
</li>
<li><p>Coordinator HA構成を組める</p></li>
<li>BIツールにはODBC/JDBCドライバが必要。ただ一から実装は大変。ということで“Prestogres”を作った</li>
</ul>
</li>
<li><p>3.クエリ実行</p>

<ul>
<li>Presto自身はデータベースではない。データストアに対してSQLを発行するエンジン（MapReduceではない）</li>
<li>実行モデルはDAGベース</li>
<li>全てのタスクはパラレルに実行され、データはメモリからメモリに引き渡される</li>
</ul>
</li>
<li><p>4.モニタリング・設定</p>

<ul>
<li>モニタリングが充実している。JMX HTTP APIで詳細な情報を取得できる。</li>
</ul>
</li>
<li><p>5.ロードマップ</p></li>
<li><p>QA</p>

<ul>
<li>同様なプロダクトであるImpalaとの性能比較

<ul>
<li>現状ではImpalaの方が速い</li>
<li>Impalaとの違いは、クエリが落ちてもプロセスが落ちない</li>
<li>ログが取りやすい。Prestoは運用周りがよく考慮されている</li>
<li>開発体制がオープン。pullreqもすぐに取り込まれる</li>
</ul>
</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GitHub Pages + Octopress]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/08/github-pages-plus-octopress/"/>
    <updated>2014-08-08T21:21:48+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/08/github-pages-plus-octopress</id>
    <content type="html"><![CDATA[<p>Octopress で　GitHub Pages を作ってみたのでメモ。</p>

<!-- more -->


<p>このへんとか、</p>

<p><a href="http://morizyun.github.io/blog/octopress-gitpage-minimum-install-guide/">OctopressでGitHub無料ブログ構築。sourceをBitbucket管理。簡単ガイド！ - 酒と泪とRubyとRails</a></p>

<p>このへんとか、</p>

<p><a href="http://qiita.com/syui/items/07365ed24eef63602233">GitHub Pages + Octopress カスタマイズ - Qiita</a></p>

<p>を参考にした。</p>

<h2>GitHubにリポジトリを作る</h2>

<p>自分のGitHubにログインして、GitHub PagesのGitHubリポジトリを作成する。
リポジトリ名は 「username.github.io」　でusernameのところには自分のユーザー名を入れる。</p>

<h2>Octopressのインストール</h2>

<p>以下の手順でOctopressのインストール&amp;GitHubへのdeploy設定を行う。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git clone git@github.com:imathis/octopress.git YoshihitoAso.github.io
</span><span class='line'>$ gem install bundler
</span><span class='line'>
</span><span class='line'>$ bundle install
</span><span class='line'>$ bundle exec rake install
</span><span class='line'>$ bundle exec rake setup_github_pages
</span><span class='line'> Enter the read/write url for your repository: git@github.com: username/username.github.com.git
</span><span class='line'> git@github.com:YoshihitoAso/YoshihitoAso.github.io.git</span></code></pre></td></tr></table></div></figure>


<h2>sample page</h2>

<p>インストールが終わったので、サンプルページを作ってみる。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ rake new_post['sample_page']</span></code></pre></td></tr></table></div></figure>


<p>でOK。すると、./source/_posts 配下に新しい *.markdown ファイルが出来ているはず。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ rake preview</span></code></pre></td></tr></table></div></figure>


<p>でプレビューを見れる。<a href="http://localhost:4000">http://localhost:4000</a> で見れる。</p>

<h2>テーマを変更する</h2>

<p>テーマを変更してみようかという気になる。
以下は<a href="https://github.com/bijumon/oct2">oct2テーマ</a>を適用する方法。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git clone git@github.com:bijumon/oct2.git .themes/oct2
</span><span class='line'>$ bundle exec rake install['oct2']
</span><span class='line'>$ rake preview</span></code></pre></td></tr></table></div></figure>


<h2>GitHub Pages へ記事をアップ</h2>

<p>最後に記事をdeployする。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd source
</span><span class='line'>$ rake gen_deploy</span></code></pre></td></tr></table></div></figure>


<p>とすればOK。最初は10分程経つと見れるようになるはず。</p>
]]></content>
  </entry>
  
</feed>
