<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[/var/log/aso.log]]></title>
  <link href="http://yoshihitoaso.github.io/atom.xml" rel="self"/>
  <link href="http://yoshihitoaso.github.io/"/>
  <updated>2014-08-25T22:26:32+09:00</updated>
  <id>http://yoshihitoaso.github.io/</id>
  <author>
    <name><![CDATA[Yoshihito Aso]]></name>
    <email><![CDATA[yoshihito.asoh@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[pyqueryでWebスクレイピング]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/25/pyquery/"/>
    <updated>2014-08-25T20:15:05+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/25/pyquery</id>
    <content type="html"><![CDATA[<p>pyqueryを使ってスクレイピングしてみた時のメモ。以下のサイトを参考にした。</p>

<p>1) <a href="http://tnakamura.hatenablog.com/entry/20110602/python_scraping_pyquery">PyQuery で再びスクレイピング入門 - present</a></p>

<p>2) GitHub: <a href="https://github.com/gawel/pyquery/">gawel/pyquery</a></p>

<h2>インストール</h2>

<p>pipでインストール</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>pip install pyquery</span></code></pre></td></tr></table></div></figure>


<p>でOK。pyqueryが依存しているlxmlもあわせてインストールされる。</p>

<h2>使い方</h2>

<p>使い方は簡単で、jquery的に、</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>from pyquery import PyQuery
</span><span class='line'>q = PyQuery(url='http://b.hatena.ne.jp/')
</span><span class='line'>
</span><span class='line'>for elem in q.find('a.entry-link'):
</span><span class='line'>#PyQuery
</span><span class='line'>    q2 = PyQuery(elem)
</span><span class='line'>    print q2.text()
</span><span class='line'>    print q2.attr('href')
</span><span class='line'>
</span><span class='line'>#lxml
</span><span class='line'>#    print elem.text
</span><span class='line'>#    print elem.get('href')</span></code></pre></td></tr></table></div></figure>


<p>こんな感じで使える。</p>

<p>短いけど以上。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[LL DIVER 参加メモ]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/25/ll-diver-2014/"/>
    <updated>2014-08-25T19:52:58+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/25/ll-diver-2014</id>
    <content type="html"><![CDATA[<p>2014/08/23 にお台場の日本科学未来館で開催された 『<a href="http://ll.jus.or.jp/2014/">LL DIVER</a>』　に参加して来た。こちらはその時のメモ。</p>

<p>※資料がまだ全て公開されていないようだったので、資料が公開され次第、随時更新していくつもり。</p>

<p><img src="https://s3-ap-northeast-1.amazonaws.com/y-asoh.aws-demo.net/lldiver.logo_.jpg" title="lldiver" alt="lldiver" /></p>

<!-- more -->


<p>当日のタイムテーブルは<a href="http://ll.jus.or.jp/2014/timetable">こちら</a>から。</p>

<p>自分が聞いたのは以下の公演。</p>

<ul>
<li>○○ as Code</li>
<li>HerokuでGauche</li>
<li>Guraプログラミング言語の紹介</li>
<li>PythonによるWebスクレイピング入門</li>
<li>エディタ対決(仮)</li>
<li>LT</li>
</ul>


<h1>○○ as Code</h1>

<p>パネリストは以下の4名</p>

<ul>
<li>高野祥幸さん</li>
<li>前佛雅人さん @zembutsu</li>
<li>吉田賢造さん</li>
<li>司会：佐々木健さん @sasakipochi</li>
</ul>


<h2>インフラのCode化：高野さん（ニフティクラウド）</h2>

<p>インフラのCode化</p>

<ul>
<li>インフラ as Code：　インスタンス起動など</li>
<li>ミドルウェア as Code：　DBをたてる（バックアップの設定なども可能）など</li>
<li>バックエンド as Code：　よりインフラを隠蔽化されていたり</li>
<li>例えば、ニフティクラウドではインフラをコード（サンプルはJavaだった）から出来るようになっている</li>
</ul>


<p>システム開発の各工程（要件定義、設計、開発、テスト、リリース）をCode化</p>

<ul>
<li>各工程はコード化されている</li>
<li>設計：今だとテスト駆動（TDD）でやったりする</li>
<li>開発：色々な言語で開発する</li>
<li>テスト：SeleniumとかXUnitとか、コードを書いてテストする</li>
<li>リリース：AnsibleやChefでコード化出来る</li>
<li>唯一要件定義工程がコード化されていない。</li>
<li>無いならなくせば？という発想で今考え始めているところ。答えはまだ無い。</li>
</ul>


<h2>農業のCode化：前佛さん</h2>

<p>現在の農業ソリューションは高価である。
農業分野にもOSSやクラウドコンピューティングの考えが導入出来ないか？と考えている。</p>

<p>運用監視の自動化</p>

<ul>
<li>昔に比べてサービスが落ちた時の影響度が甚大である</li>
<li>全て人手では難しい。運用の自動化・Code化。</li>
<li>運用監視としてMuninやZabix、オーケストレーションツール（Serf、Consul）などで自動化したり。</li>
<li>運用のコード化の流れ

<ul>
<li>作業手順書作成</li>
<li>監視ツール導入</li>
<li>自動化技術の導入</li>
</ul>
</li>
</ul>


<p>農業への適用を考えたい</p>

<ul>
<li>牛→餌の食わない牛（機械）へ</li>
<li>なぜ農業で自動化・コード化が必要？

<ul>
<li>人が足りてない</li>
<li>商品での差別化は難しい</li>
</ul>
</li>
<li>重要になってくる分野

<ul>
<li>情報化、自動化、ロボット技術</li>
<li>流れの中でslack, hubotを利用してみたら良いかもみたいな話があった。</li>
</ul>
</li>
</ul>


<h2>工業のCode化：吉田さん（株式会社スマメ代表など）</h2>

<p>3Dプリンタ関連の導入支援を行っている（DMM.makeとか）。その他にしぶや図工室、Fabbitなど。</p>

<p>3Dプリンタ業界のぶっちゃけ話</p>

<ul>
<li>プレーヤーがずっと変わってない（市場的に成熟してしまっている）</li>
<li>金額が色々とおかしい（日本では価格が高すぎる）</li>
<li>そもそも思っている程大したものではない

<ul>
<li>出来る形状と出来ない形状がある</li>
<li>独自の素材しか使えない</li>
<li>材料費が高い</li>
<li>データ形式（stl）が汎用的ではない（かなり前から存在する形式）</li>
</ul>
</li>
</ul>


<p>3Dプリンタが使えると思われる分野</p>

<ul>
<li>補聴器</li>
<li>義歯、義足、人工骨/インプラント</li>
</ul>


<h2>ディスカッション</h2>

<p>Googleカーみたいなものは農業分野で役立つか？みたいな話があった。
これについては、まだやや飛躍気味で、機械学習が出来るような知見がたまるまでに少し時間がかかるだろうという話であった。それよりももっと細かいところでの改善がまだまだ必要で、田んぼに引き込む水量の制御など、自動化出来るところがまだまだあるのでは？といった話がされていた。</p>

<p>農業分野に3Dプリンタが生かせないか？みたいな話があった。</p>

<ul>
<li>壊れても良い、安価な機械を３Dプリンタで作ることが出来たらおもしろいかも</li>
<li>情報を公開することで、ノウハウの共有、オープンソース化が出来る。</li>
</ul>


<h1>HerokuでGauche (あるいは、好きな言語何でも)：久井亨さん</h1>

<p>Herokuは色々な言語が使える。ただし、GaucheのようなScheme処理系はサポートされていない。</p>

<p>Herokuの内部構造</p>

<ul>
<li>App

<ul>
<li>Cedar Stack　（ベースOS Ubuntu 10.4）

<ul>
<li>Dyno</li>
</ul>
</li>
</ul>
</li>
</ul>


<p>HerokuでScheme処理系言語を使いたいなら、Cedar Stack上でSlugを利用すればOK。</p>

<p>参考：<a href="http://torus.tumblr.com/post/87300539228/heroku-gauche">Heroku で Gauche のアプリケーションを動かす</a></p>

<p>ただし、Ubuntu10.04でコンパイルする必要がある。Dockerを使うと便利。</p>

<h1>PythonによるWebスクレイピング入門：関根裕紀さん</h1>

<p>Webスクレイピングの方法</p>

<ul>
<li>1)Webサービスを使う（Yahoo! Pipesとか）</li>
<li>2)プログラム言語を使う</li>
</ul>


<p>pythonの場合は以下の選択肢がある
-   標準ライブラリ　urllib2
-   beautiful soup
-   pyquery
-   scrapy</p>

<p>それぞれ特徴があり、できる事も違う。用途によって使い分ける感じになると思う。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Clear Excel Formats]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/25/clear-excel-formats/"/>
    <updated>2014-08-25T00:00:00+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/25/clear-excel-formats</id>
    <content type="html"><![CDATA[<p>仕事柄Excelを使うことが多いです。</p>

<p>最近も回ってきた資料を更新していたら、列をコピーして挿入などすると、</p>

<p>「セルの書式が多すぎるため、書式を追加できません」</p>

<p>のエラーが出まくって暫く白目をむいたりした。</p>

<p>そんな時のメモ。</p>

<!-- more -->


<p>ネットで調べたところ、原因としては新しい書式設定をするたびにExcelがそのセルの書式パターンをスタイルとして過去分も蓄積するためらしい。これが例えばExcel2007であれば、64000パターンを超えると上記のエラーが出るようだ。</p>

<p>Excelのホームタブで「スタイル」のところのプルダウンに大量に書式が出てきたら、このエラーが出やすくなっている資料なので、使い回す際は気をつけろということになる。</p>

<p>過去分の書式はマクロを利用して消す事ができる（Excelの標準機能は画面から一つずつ消していくしかない）。</p>

<p>削除マクロの例は以下の記事にありました。ありがとうございます。</p>

<p><a href="http://www.ixam.net/ms-excel/vba-bian/delete-style">書式（スタイル）と名前の定義をすべて削除する - ixam</a></p>

<p>シートの一番下のタブを右クリックして、「コードの表示」を選択して、マクロのソースに以下のコードを貼り付けて、実行すると過去の書式が消える。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Sub delete_name_and_style()
</span><span class='line'>
</span><span class='line'>    On Error Resume Next
</span><span class='line'>
</span><span class='line'>'名前定義を全削除（名前を関数その他に有効活用している場合はここは削除）
</span><span class='line'>
</span><span class='line'>    Dim N As Name
</span><span class='line'>    For Each N In ActiveWorkbook.Names
</span><span class='line'>        N.Delete
</span><span class='line'>    Next
</span><span class='line'>
</span><span class='line'>'書式（スタイル）定義を全削除
</span><span class='line'>
</span><span class='line'>    Dim M()
</span><span class='line'>
</span><span class='line'>    J = ActiveWorkbook.Styles.Count
</span><span class='line'>    ReDim M(J)
</span><span class='line'>    For i = 1 To J
</span><span class='line'>        M(i) = ActiveWorkbook.Styles(i).Name
</span><span class='line'>    Next
</span><span class='line'>    For i = 1 To J
</span><span class='line'>        If InStr("Hyperlink,Normal,Followed Hyperlink", _
</span><span class='line'>                    M(i)) = 0 Then
</span><span class='line'>            ActiveWorkbook.Styles(M(i)).Delete
</span><span class='line'>        End If
</span><span class='line'>    Next
</span><span class='line'>
</span><span class='line'>End Sub</span></code></pre></td></tr></table></div></figure>


<p>このコードは名前定義も削除するコードなので、必要無ければ該当部分は消しても良い。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[UbuntuにMongoDBをインストールする]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/20/ubuntu-mongo/"/>
    <updated>2014-08-20T15:54:27+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/20/ubuntu-mongo</id>
    <content type="html"><![CDATA[<p>UbuntuにMongoDBをインストールしたときのメモ。</p>

<p>※参考情報：OSのバージョン</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat /etc/lsb-release
</span><span class='line'>DISTRIB_ID=Ubuntu
</span><span class='line'>DISTRIB_RELEASE=12.10
</span><span class='line'>DISTRIB_CODENAME=quantal
</span><span class='line'>DISTRIB_DESCRIPTION="Ubuntu 12.10"</span></code></pre></td></tr></table></div></figure>


<h2>MongoDBをインストール</h2>

<p>公式ドキュメントの通り、以下の手順でインストール出来る</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10
</span><span class='line'>$ echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' | sudo tee /etc/apt/sources.list.d/mongodb.list
</span><span class='line'>$ sudo apt-get update
</span><span class='line'>$ sudo apt-get install mongodb-10gen</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>MongoDBの起動</h2>

<p>起動停止は以下のコマンドで実施する。</p>

<p>MongoDBのプロセスを開始</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo service mongodb start</span></code></pre></td></tr></table></div></figure>


<p>MongoDBのプロセスを停止</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo service mongodb stop</span></code></pre></td></tr></table></div></figure>


<p>MongoDBのプロセスを再起動</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo service mongodb restart</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu上にCDH4（Hadoop）擬似分散モード環境を構築する （※おまけでHiveをインストール）]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/20/cdh4-pseudo-distributed-mode/"/>
    <updated>2014-08-20T15:44:13+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/20/cdh4-pseudo-distributed-mode</id>
    <content type="html"><![CDATA[<p>Ubuntuサーバ上にCDH4を擬似分散モードでインストールした時のメモ。</p>

<p>※参考情報：OSのバージョン</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cat /etc/lsb-release
</span><span class='line'>DISTRIB_ID=Ubuntu
</span><span class='line'>DISTRIB_RELEASE=12.10
</span><span class='line'>DISTRIB_CODENAME=quantal
</span><span class='line'>DISTRIB_DESCRIPTION="Ubuntu 12.10"</span></code></pre></td></tr></table></div></figure>




<!-- more -->


<h2>Oracle-Javaのインストール</h2>

<p>Java7をインストールする。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo add-apt-repository ppa:webupd8team/java
</span><span class='line'>$ sudo apt-get update
</span><span class='line'>$ sudo apt-get install oracle-java7-installer</span></code></pre></td></tr></table></div></figure>


<p>バージョンの確認</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ java -version
</span><span class='line'>java version "1.7.0_51"
</span><span class='line'>Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
</span><span class='line'>Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)
</span><span class='line'>$ javac -version
</span><span class='line'>javac 1.7.0_51</span></code></pre></td></tr></table></div></figure>


<p>pathにJAVA_HOMEを設定しておく必要がある。
.bashrcに以下の設定を追記。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>export JAVA_HOME=/usr/lib/jvm/java-7-oracle
</span><span class='line'>export PATH=$PATH:$JAVA_HOME/bin</span></code></pre></td></tr></table></div></figure>


<h2>CDH4のインストール</h2>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ wget http://archive.cloudera.com/cdh4/one-click-install/precise/amd64/cdh4-repository_1.0_all.deb
</span><span class='line'>$ sudo dpkg -i cdh4-repository_1.0_all.deb
</span><span class='line'>$ curl -s http://archive.cloudera.com/cdh4/ubuntu/precise/amd64/cdh/archive.key | sudo apt-key add -
</span><span class='line'>
</span><span class='line'>$ sudo apt-get update
</span><span class='line'>$ sudo apt-get install hadoop-conf-pseudo</span></code></pre></td></tr></table></div></figure>


<p>ディレクトリを確認。各種設定ファイルが格納されている。
通常起動の場合は編集の必要はない。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ ls -l /etc/hadoop/conf.pseudo
</span><span class='line'>total 40
</span><span class='line'>-rw-r--r-- 1 root hadoop 1458 Feb 26 09:54 core-site.xml
</span><span class='line'>-rw-r--r-- 1 root hadoop 1364 Feb 26 09:54 hadoop-env.sh
</span><span class='line'>-rw-r--r-- 1 root hadoop 2890 Feb 26 09:54 hadoop-metrics.properties
</span><span class='line'>-rw-r--r-- 1 root hadoop 1875 Feb 26 09:54 hdfs-site.xml
</span><span class='line'>-rw-r--r-- 1 root hadoop 8735 Feb 26 10:21 log4j.properties
</span><span class='line'>-rw-r--r-- 1 root hadoop 1549 Feb 26 09:54 mapred-site.xml
</span><span class='line'>-rw-r--r-- 1 root hadoop 1104 Feb 26 09:54 README
</span><span class='line'>-rw-r--r-- 1 root hadoop 2361 Feb 26 09:54 yarn-site.xml</span></code></pre></td></tr></table></div></figure>


<h2>HDFS /tmp directory を作成する</h2>

<p>hdfsユーザでHDFSのフォーマットを実施する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo -u hdfs
</span><span class='line'>$ hdfs namenode -format</span></code></pre></td></tr></table></div></figure>


<p>HDFSを起動。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ /etc/init.d/hadoop-hdfs-namenode
</span><span class='line'>$ /etc/init.d/hadoop-hdfs-datanode
</span><span class='line'>$ /etc/init.d/hadoop-hdfs-secondarynamenode</span></code></pre></td></tr></table></div></figure>


<p>とすれば良いけど、毎回このコマンドを打つのは面倒なので以下の様なスクリプト（hadoop-hdfs-start）を作っておく。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>for service in /etc/init.d/hadoop-hdfs-*
</span><span class='line'>do
</span><span class='line'>sudo $service start
</span><span class='line'>done</span></code></pre></td></tr></table></div></figure>


<p>停止の場合も同様。以下の様なスクリプト（hadoop-hdfs-stop）を作っておくとラク。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>#!/bin/bash
</span><span class='line'>for service in /etc/init.d/hadoop-hdfs-*
</span><span class='line'>do
</span><span class='line'>sudo $service stop
</span><span class='line'>done</span></code></pre></td></tr></table></div></figure>


<p>起動を実行したら、各サービスが起動していることを確認する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo $JAVA_HOME/bin/jps
</span><span class='line'>4233 Jps
</span><span class='line'>2476 NodeManager
</span><span class='line'>2007 SecondaryNameNode
</span><span class='line'>2622 ResourceManager
</span><span class='line'>1827 NameNode
</span><span class='line'>1652 DataNode</span></code></pre></td></tr></table></div></figure>


<h2>必要なDirectoryを作成する</h2>

<p>HDFS上に必要なディレクトリを作成していく。この作業もhdfsユーザで実施する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo su - hdfs
</span><span class='line'>$ hadoop fs -mkdir /tmp　←HDFS上に/tmpを作成
</span><span class='line'>$ hadoop fs -chmod -R 1777 /tmp
</span><span class='line'>$ hadoop fs -mkdir /var/log/hadoop-yarn
</span><span class='line'>$ hadoop fs -chown yarn:mapred /var/log/hadoop-yarn
</span><span class='line'>$ hadoop fs -mkdir /tmp/hadoop-yarn/staging
</span><span class='line'>$ hadoop fs -chmod -R 1777 /tmp/hadoop-yarn/staging
</span><span class='line'>$ hadoop fs -mkdir /tmp/hadoop-yarn/staging/history/done_intermediate
</span><span class='line'>$ hadoop fs -chmod -R 1777 /tmp/hadoop-yarn/staging/history/done_intermediate
</span><span class='line'>$ hadoop fs -mkdir /user/$USER　←hdfsユーザのホームディレクトリ/user/hdfs
</span><span class='line'>$ hadoop fs -chown hdfs /user/$USER</span></code></pre></td></tr></table></div></figure>


<p>/user/hdfsが作業ディレクトリと考えてよい。上記実行中エラーが出て中断するようなら、HDFSが落ちている可能性があるので起動状態を確認してみる。</p>

<p>HDFS上のlsは以下のようにして実行する。上記で作ったtmp,user,varができてればOK。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ hadoop fs -ls /
</span><span class='line'>Found 3 items
</span><span class='line'>drwxrwxrwt – hdfs supergroup 0 2013-01-05 12:10 /tmp
</span><span class='line'>drwxr-xr-x – hdfs supergroup 0 2013-01-05 12:36 /user
</span><span class='line'>drwxr-xr-x – hdfs supergroup 0 2013-01-05 12:09 /var</span></code></pre></td></tr></table></div></figure>


<h2>Start YARN !!!!!</h2>

<p>管理ユーザにスイッチしてYARN (MapReduce)を起動する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo service hadoop-yarn-resourcemanager start
</span><span class='line'>$ sudo service hadoop-yarn-nodemanager start
</span><span class='line'>$ sudo service hadoop-mapreduce-historyserver start</span></code></pre></td></tr></table></div></figure>


<p>プロセスの確認。以下の様な感じになっていればOK。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ sudo $JAVA_HOME/bin/jps
</span><span class='line'>2598 ResourceManager
</span><span class='line'>4798 JobHistoryServer
</span><span class='line'>4840 Jps
</span><span class='line'>1752 NameNode
</span><span class='line'>2448 NodeManager
</span><span class='line'>2043 SecondaryNameNode
</span><span class='line'>1496 DataNode</span></code></pre></td></tr></table></div></figure>


<h1>おまけ）Hiveをインストールして使ってみる</h1>

<h2>Hiveのインストール</h2>

<p>以下の手順でHiveをインストールする。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd Downloads/
</span><span class='line'>$ mkdir hive
</span><span class='line'>$ cd hive/
</span><span class='line'>$ wget http://mirror.tcpdiag.net/apache/hive/stable/hive-0.11.0.tar.gz
</span><span class='line'>$ tar xzf hive-0.11.0.tar.gz
</span><span class='line'>$ mkdir /usr/lib/hive
</span><span class='line'>$ mv hive-0.11.0 /usr/lib/hive/hive-0.11.0</span></code></pre></td></tr></table></div></figure>


<p>環境変数を設定する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd
</span><span class='line'>$ vim .bashrc
</span><span class='line'>$ export HIVE_HOME=/usr/lib/hive/hive-0.11.0
</span><span class='line'>$ export PATH=$PATH:$HIVE_HOME/bin</span></code></pre></td></tr></table></div></figure>


<h3>Login</h3>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ hive
</span><span class='line'>...
</span><span class='line'>..
</span><span class='line'>.
</span><span class='line'>
</span><span class='line'>hive&gt;</span></code></pre></td></tr></table></div></figure>


<p>でOK。</p>

<h2>Sample : Create Table</h2>

<p>Tableを作成して、CSV形式のファイルをロードしてみる。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hive&gt; create table cats (id int, name string, birthday string) row format delimited fields terminated by ',' lines terminated by '\n';</span></code></pre></td></tr></table></div></figure>


<p>csv形式のファイルを以下のように用意する。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ vim cats.csv
</span><span class='line'>
</span><span class='line'>1,Tama,19929423
</span><span class='line'>2,Tora,19930304
</span><span class='line'>3,Nya,19930304
</span></code></pre></td></tr></table></div></figure>


<p>以下のコマンドでロード出来る。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>hive&gt; load data local inpath '/root/hadoop/hive/cats.csv' overwrite into table cats;</span></code></pre></td></tr></table></div></figure>



]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[straceでjavaプロセスの挙動を確認した]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/11/strace/"/>
    <updated>2014-08-11T11:03:25+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/11/strace</id>
    <content type="html"><![CDATA[<h2>strace コマンドとは</h2>

<p>この辺が参考になる。</p>

<blockquote><p><a href="http://blog.livedoor.jp/sonots/archives/18193659.html">strace コマンドの使い方をまとめてみた - sonots:blog</a></p></blockquote>

<p>straceを利用して、プロセスが呼び出すシステムコールをトレースすることが出来る。</p>

<h2>コマンドからトレース</h2>

<p>実行コマンドが分かっている場合は、strace の後にトレースしたいコマンドを記述する感じ。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ strace [command]</span></code></pre></td></tr></table></div></figure>


<p>という感じで実行すれば良い。</p>

<h2>プロセスを指定</h2>

<p>実行中プロセスをトレースしたい場合がある。pid 指定でトレースする。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>strace -p [pid]</span></code></pre></td></tr></table></div></figure>


<h2>具体例</h2>

<p>例が渋すぎるが、最近実行した例を1つ。
JBossのtwiddleがある環境でロングランする事象があったので調査した。
切り分けし易いように、twiddle.shではなく、内部で実行しているjavaコマンドを直接実行してトレースした。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>strace -tt -f -s 1024 -o strace.log java -classpath /&lt;任意のpath&gt;/twiddle.jar:/&lt;任意のpath&gt;/jbossall-client.jar:/&lt;任意のpath&gt;/jboss-common.jar:/&lt;任意のpath&gt;/getopt.jar:/&lt;任意のpath&gt;/log4j.jar:/&lt;任意のpath&gt;/jboss-jmx.jar:/&lt;任意のpath&gt;/dom4j-1.5.jar org.jboss.console.twiddle.Twiddle -s &lt;任意のurlやip&gt;:&lt;任意のport&gt; &lt;command&gt; [ command_arguments</span></code></pre></td></tr></table></div></figure>


<p>オプションが幾つかついているが補足しておくと、</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>-tt: マイクロ秒単位で表示
</span><span class='line'>-f: 内部でスレッドが切れている場合、スレッドまでトレースする場合
</span><span class='line'>-s: 1行あたり表示される文字数を変更する。（デフォルトは32文字）
</span><span class='line'>-o: ファイルに出力するオプション</span></code></pre></td></tr></table></div></figure>


<p>といった感じ。</p>

<p>-f オプションの場合、別プロセスのトレース結果が同一ファイルに出力される。別ファイルにしたい場合は、-ff オプションにすれば良い。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hadoop Conference Japan 2014　参加メモ #hcj2014]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/08/hadoop-conference-japan-2014-number-hcj2014/"/>
    <updated>2014-08-08T21:49:15+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/08/hadoop-conference-japan-2014-number-hcj2014</id>
    <content type="html"><![CDATA[<p><img src="https://s3-ap-northeast-1.amazonaws.com/y-asoh.aws-demo.net/logo.png" title="hadoop" alt="hadoop" /></p>

<p>メモは残してそのままにしていたが、こちらに書いておくことにする。</p>

<!-- more -->


<p>Hadoop Conference Japan 2014 に参加してきたので、その参加メモ。</p>

<p>7/8に開催された Hadoop Conference Japan 2014に参加してきた（去年に引き続き2回目の参加）。</p>

<p>そもそもHadoop Conference Japanとは、並列分散処理フレームワーク Apache Hadoop および周辺のオープンソースソフトウェアに関するユーザカンファレンス。
日本Hadoopユーザー会の有志によって運営されており、今回で5回目の開催になる。</p>

<p>↓各種レポート記事へのリンク↓</p>

<ul>
<li><strong>gihyo.jp</strong>

<ul>
<li><a href="http://gihyo.jp/news/report/2014/07/0902">日本よ，これが2014年のHadoopだ！─「Hadoop Conference Japan 2014」基調講演レポート</a></li>
</ul>
</li>
<li><strong>ITpro</strong>

<ul>
<li><a href="http://itpro.nikkeibp.co.jp/article/NEWS/20140708/569985/">「Hadoopはビッグデータの“OSカーネル”」、Hadoop Conference Japan開催</a></li>
<li><a href="http://itpro.nikkeibp.co.jp/article/COLUMN/20140711/570863/">NTTデータが4000コアのクラスターでSparkを試行、NTTドコモからの要望受け</a></li>
</ul>
</li>
<li><strong>Publickey</strong>

<ul>
<li><a href="http://www.publickey1.jp/blog/14/yarnhadoophadoop_conference_japan_2014.html">YARNの登場によりHadoopは複数の並列分散処理エンジンを併用できる環境へ</a></li>
<li><a href="http://www.publickey1.jp/blog/14/hadoopoltphadoop_conference_japan_2014.html">HadoopはいずれOLTPも実現し、エンタープライズデータハブとなる</a></li>
</ul>
</li>
</ul>


<h1>タイムテーブル</h1>

<p>当日のタイムテーブルは<a href="https://www.eventbrite.com/e/hadoop-conference-japan-2014-tickets-12016613013">こちら</a></p>

<p>午前のキーノートから始まり、昼食（無料！）を挟んで、午後はHadoopに関する技術トピックや活用事例を紹介する発表、という流れ。</p>

<p>以下は、自分が聞いた発表</p>

<p>【午前：キーノート】</p>

<ul>
<li>米谷 修 （リクルートテクノロジーズ）、濱野 賢一朗 （日本Hadoopユーザー会, NTTデータ）</li>
<li>Doug Cutting （Hadoop生みの親, Apache Software Foundation, Cloudera） 『The Future of Data』</li>
<li>Patrick Wendell （Apache Spark主要開発者, Databricks） 『The Future of Spark』 [講演資料]</li>
<li>太田 一樹 （Treasure Data CTO） 『Hadoopエコシステムの変遷と、見えてきた使いどころ』</li>
</ul>


<p>【午後】</p>

<ul>
<li>リクルート式Hadoopの使い方 3rd Edition : 石川 信行（リクルートテクノロジーズ）</li>
<li>SQLによるバッチ処理とストリーム処理 : 田籠 聡 (LINE)</li>
<li>A Deeper Understanding of Spark Internals : Patrick Wendell （Databricks）</li>
<li>Spark1.0での動作検証 - Hadoopユーザ・デベロッパから見たSparkへの期待 : 土橋 昌 （NTTデータ）</li>
<li>Treasure Data on The YARN : 小林 隆（Treasure Data）</li>
<li>並列SQLエンジンPresto - 大規模データセットを高速にグラフ化する方法 : 古橋 貞之（Treasure Data）</li>
</ul>


<h1>午前：キーノート</h1>

<h2>濱野 賢一朗 （日本Hadoopユーザー会, NTTデータ）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36781484" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/hamaken/hadoop-conference-japan-2014-36781484" title="Hadoop Conference Japan 2014 ご挨拶・Hadoopを取り巻く環境" target="_blank">Hadoop Conference Japan 2014 ご挨拶・Hadoopを取り巻く環境</a> </strong> from <strong><a href="http://www.slideshare.net/hamaken" target="_blank">hamaken</a></strong> </div></p>

<ul>
<li>今回の参加者数は1296名。そのうち約65%が初参加とのこと。</li>
<li>Hadoopは絶賛進化中。現在は2.X系の開発が中心</li>
<li>その中心的なトピックは「YARN」（※）</li>
<li>日本におけるHadoopの利用ユーザは徐々に増えており、今回の参加者の44%以上が6ヶ月以上の利用経験を有する</li>
<li>利用しているエコシステムとしてはHive、HBaseが多い。Impala、Sparkなどの比較的新しいプロダクトについても利用ユーザは相当数いる。</li>
</ul>


<blockquote><p><strong>YARN（Yet Another Resource Negotiator）についての参考資料</strong></p>

<ul>
<li>Apache Hadoop 2.4.1 - YARN <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html</a></li>
<li>YARN の紹介 <a href="http://www.ibm.com/developerworks/jp/bigdata/library/bd-yarn-intro/">http://www.ibm.com/developerworks/jp/bigdata/library/bd-yarn-intro/</a></li>
</ul>
</blockquote>

<h2>Doug Cutting （Hadoop生みの親、Apache Software Foundation, Cloudera）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36826369" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/Cloudera_jp/the-future-of-data-jp" title="The future of data by Doug Cutting #hcj2014" target="_blank">The future of data by Doug Cutting #hcj2014</a> </strong> from <strong><a href="http://www.slideshare.net/Cloudera_jp" target="_blank">Cloudera Japan</a></strong> </div></p>

<ul>
<li>ハードウェアの価格は更に安く、データの価値は更に高まる（よりハイレゾリューションに）、そのためのソフトウェアが必要になる</li>
<li>そしてオープンソースが勝ち残る (luceneは 1999 から開発開始しているが、事実生き残っている）</li>
<li>オープンソースは使う側から見てもリスクを低減する。プラットフォーム技術については特にOSSであることが必要とされる。</li>
<li>Hadoopの機能はさらに向上し、Hadoopが当たり前になるだろう</li>
<li>GoogleのSpannerのようにトランザクション処理をHadoop上でサポートするという話があった（※）</li>
</ul>


<blockquote><p><strong>※参考記事</strong></p>

<ul>
<li>Hadoop creator: &lsquo;Google is living a few years in the future and sending the rest of us messages&rsquo; | ZDNet <a href="http://www.zdnet.com/hadoop-creator-google-is-living-a-few-years-in-the-future-and-sending-the-rest-of-us-messages-7000023160/">http://www.zdnet.com/hadoop-creator-google-is-living-a-few-years-in-the-future-and-sending-the-rest-of-us-messages-7000023160/</a></li>
</ul>
</blockquote>

<h2>Patrick Wendell （Apache Spark主要開発者、 Databricks）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36856077" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/hadoopconf/the-future-of-apache-spark" title="The Future of Apache Spark" target="_blank">The Future of Apache Spark</a> </strong> from <strong><a href="http://www.slideshare.net/hadoopconf" target="_blank">Hadoop Conference Japan</a></strong> </div></p>

<ul>
<li>Spark開発者の一週間の活動

<ul>
<li>パッチの投稿や修正等：500件、JIRA/GitHub上でのコメント：200件、メール：140スレッド、マージされるパッチの数：80件</li>
</ul>
</li>
<li>Sparkはここ1年で急速に成長した</li>
<li>APIの安定性を重視している。APIを破壊するようなパッチはビルドに失敗するようにしている</li>
<li>開発者にやさしいリリースサイクルでリリースを行っている。マイナーリリースは3ヶ月毎、必要に応じてメンテナンスリリースを行っている。</li>
<li>Sparkの主要エコシステムはSpark SQL（SQL on Spark）、MLLib（機械学習ライブラリ）、GraphX（グラフDB）、Spark Streaming（ストリーム処理）</li>
<li>そのうちSpark SQLは他のコンポーネントよりも早く成長している。MLLibがその次。</li>
<li>SparkR といプロダクトもある。RでSparkを動かす。</li>
<li>Databricks Cloudのデモ。Web UI上でSparkのプログラムが書ける。SQLも実行できる。その実行結果を簡単にグラフ表示ができる。すごい。</li>
</ul>


<blockquote><p><strong>Apache Spark に関する参考資料</strong></p>

<ul>
<li>Apache Spark の紹介（前半：Sparkのキホン） <a href="http://www.slideshare.net/hadoopxnttdata/apache-spark-spark">http://www.slideshare.net/hadoopxnttdata/apache-spark-spark</a></li>
<li>Apache Sparkのご紹介 （後半：技術トピック） <a href="http://www.slideshare.net/hadoopxnttdata/apache-spark">http://www.slideshare.net/hadoopxnttdata/apache-spark</a></li>
<li>Spark: 高速なデータ分析のための新たな手段 <a href="http://www.ibm.com/developerworks/jp/opensource/library/os-spark/">http://www.ibm.com/developerworks/jp/opensource/library/os-spark/</a></li>
</ul>
</blockquote>

<h2>太田 一樹 （Treasure Data CTO）</h2>

<p><strong>『Hadoopエコシステムの変遷と、見えてきた使いどころ』（資料未公開）</strong></p>

<ul>
<li>ただ単に「安いストレージ的」に使うのであれば、GlusterFSやCephといったストレージ特化の様々なプロダクトがある。こちらのほうが優れてる。</li>
<li><p><strong>Hadoop エコシステムの進化と混沌</strong> 4つの側面で様々なプロダクトが乱立している</p>

<ul>
<li>1.Collect Any Types of data

<ul>
<li>様々なデータ収集ミドルウェアの開発が進んでいる</li>
<li>Fluentd, Kafka, Flume, Sqoop</li>
</ul>
</li>
<li>2.Store Any types of data economically

<ul>
<li>Parquet, ORCFile (format)</li>
<li>HDFS, HBase, Accumulo</li>
<li>Ambali, HUE, Cluudera Maager 管理/運用の支援は大事</li>
<li>Treasure Data, AWS EMR</li>
</ul>
</li>
<li>3.Faster Use of Data

<ul>
<li>いかに早くデータを扱うか？</li>
<li>YARN</li>
<li>Storm, Samza, Norikra</li>
<li>Apache Tez, Spark</li>
<li>HiveQL, Pig</li>
<li>Java: Cascading, Apache Crunch</li>
</ul>
</li>
<li>4.Better Use of Data

<ul>
<li>いかにうまくデータを扱うか？</li>
<li>SQL on Hadoop：Impala, SparkSQL, Presto, Drill</li>
<li>機械学習：Mahout, Spark MLlib, Hivemall</li>
</ul>
</li>
</ul>
</li>
<li><p>他の選択肢の進化</p>

<ul>
<li>  Database の進化

<ul>
<li>  MPP（Massiely Parallel Processing） MPPデータベース（RedShiftやBigQueryなど）　（※）</li>
<li>  Schema-on-Write （先にスキーマを決めないといけない） 。アドホックな解析をするためのデータベースとしては少し不向き。</li>
<li>  Oracle, DB2, SQLserver</li>
<li>  Teradata, Netezza, Vertica, ParAccel, Greenplum</li>
<li>  多くのベンダーが Hadoop 対応を表明</li>
<li>  基本的にはスキーマを決めないといけないが、Vertica Zonemap など Schema-on-Read 対応なども</li>
<li>  現在の主流

<ul>
<li>  Hadoop に生データをすべて集約</li>
<li>  そこから集計集約したデータを MPP データベースに保存</li>
</ul>
</li>
<li>  Hadoop は構造化データとの境界線に</li>
<li>  MPP データベースは非構造化データの領域に踏み込む</li>
<li>  誰がマーケットリーダーになっていくのか注視していかないといけない</li>
<li>  使用する側は一層の知識とトレンドの把握が必要。</li>
</ul>
</li>
</ul>
</li>
</ul>


<blockquote><p><strong>※参考資料</strong></p>

<p>MPPの使い分けの話</p>

<ul>
<li>MPP on Hadoop, Redshift, BigQuery - Go ahead! <a href="http://repeatedly.github.io/ja/2014/07/mpp-on-hadoop-redshift-bigquery/">http://repeatedly.github.io/ja/2014/07/mpp-on-hadoop-redshift-bigquery/</a></li>
</ul>
</blockquote>

<h1>午後</h1>

<h2>リクルート式Hadoopの使い方 3rd Edition : 石川 信行（リクルートテクノロジーズ）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36770927" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/recruitcojp/hadoop20140707" title="Hadoopカンファレンス20140707" target="_blank">Hadoopカンファレンス20140707</a> </strong> from <strong><a href="http://www.slideshare.net/recruitcojp" target="_blank">Recruit Technologies</a></strong> </div></p>

<ul>
<li>リクルートグループのHadoop活用事例に関する発表</li>
<li>様々な利用用途でHadoop、およびHadoopのエコシステムを利用している。詳細は資料の通り。</li>
<li>リクルートでは2010年からHadoopの研究開発、2011年から本格展開を始めた</li>
<li>2014年はアドホック分析基盤の導入等の活動を行っている</li>
<li>導入に際しては、既存サービスを生かしつつ、徐々に利用範囲を広げていっている（安く、早く提供）</li>
<li>本番提供しているサービスの他にも、現在も様々な技術検証を並行して実施している。

<ul>
<li>画像解析（スパースコーディングなど）</li>
<li>テキスト解析（Skip-Gramなど）</li>
<li>グラフ（Titanなど）</li>
</ul>
</li>
</ul>


<h2>並列SQLエンジンPresto - 大規模データセットを高速にグラフ化する方法 : 古橋 貞之（Treasure Data）</h2>

<iframe src="//www.slideshare.net/slideshow/embed_code/36771514" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/frsyuki/presto-hadoop-conference-japan-2014" title="Presto - Hadoop Conference Japan 2014" target="_blank">Presto - Hadoop Conference Japan 2014</a> </strong> from <strong><a href="http://www.slideshare.net/frsyuki" target="_blank">Sadayuki Furuhashi</a></strong> </div></p>

<ul>
<li><p>1.イントロ</p>

<ul>
<li>PrestoとはGBからPBのデータ分析を対話的に行う分散SQLクエリエンジン</li>
<li>2012秋からFacebookで開発された</li>
<li>Prestoが解決しようとする問題は

<ul>
<li>BIツールから直接HDFS上のデータを可視化できない</li>
<li>日次バッチでPostgreSQLやRedshiftにデータを入れる必要がある</li>
<li>いくつかのデータはHDFS上にはない。そのため分析時にHDFS上にコピーが必要</li>
</ul>
</li>
<li>Prestoだとクエリをミリ秒から分で処理。ただしETLにはMapReduceやHiveは必要</li>
<li>BIツールから接続可能。ODBC/JDBCコネクタ</li>
<li>複数のデータソースにまたがってクエリを実行可能</li>
</ul>
</li>
<li><p>2.分散アーキテクチャ</p>

<ul>
<li>Discovery Service</li>
<li>Coordinator</li>
<li>Worker</li>
<li><p>Connector Plugin : ConnectorはJavaで書かれており、ストレージとメタデータの実装。自作も可能。</p>

<ul>
<li>Hive Connector</li>
<li>Cassandra Connector</li>
<li>MySQL through JDBC Connector(prerelease)</li>
</ul>
</li>
<li><p>Coordinator HA構成を組める</p></li>
<li>BIツールにはODBC/JDBCドライバが必要。ただ一から実装は大変。ということで“Prestogres”を作った</li>
</ul>
</li>
<li><p>3.クエリ実行</p>

<ul>
<li>Presto自身はデータベースではない。データストアに対してSQLを発行するエンジン（MapReduceではない）</li>
<li>実行モデルはDAGベース</li>
<li>全てのタスクはパラレルに実行され、データはメモリからメモリに引き渡される</li>
</ul>
</li>
<li><p>4.モニタリング・設定</p>

<ul>
<li>モニタリングが充実している。JMX HTTP APIで詳細な情報を取得できる。</li>
</ul>
</li>
<li><p>5.ロードマップ</p></li>
<li><p>QA</p>

<ul>
<li>同様なプロダクトであるImpalaとの性能比較

<ul>
<li>現状ではImpalaの方が速い</li>
<li>Impalaとの違いは、クエリが落ちてもプロセスが落ちない</li>
<li>ログが取りやすい。Prestoは運用周りがよく考慮されている</li>
<li>開発体制がオープン。pullreqもすぐに取り込まれる</li>
</ul>
</li>
</ul>
</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GitHub Pages + Octopress]]></title>
    <link href="http://yoshihitoaso.github.io/blog/2014/08/08/github-pages-plus-octopress/"/>
    <updated>2014-08-08T21:21:48+09:00</updated>
    <id>http://yoshihitoaso.github.io/blog/2014/08/08/github-pages-plus-octopress</id>
    <content type="html"><![CDATA[<p>Octopress で　GitHub Pages を作ってみたのでメモ。</p>

<!-- more -->


<p>このへんとか、</p>

<p><a href="http://morizyun.github.io/blog/octopress-gitpage-minimum-install-guide/">OctopressでGitHub無料ブログ構築。sourceをBitbucket管理。簡単ガイド！ - 酒と泪とRubyとRails</a></p>

<p>このへんとか、</p>

<p><a href="http://qiita.com/syui/items/07365ed24eef63602233">GitHub Pages + Octopress カスタマイズ - Qiita</a></p>

<p>を参考にした。</p>

<h2>GitHubにリポジトリを作る</h2>

<p>自分のGitHubにログインして、GitHub PagesのGitHubリポジトリを作成する。
リポジトリ名は 「username.github.io」　でusernameのところには自分のユーザー名を入れる。</p>

<h2>Octopressのインストール</h2>

<p>以下の手順でOctopressのインストール&amp;GitHubへのdeploy設定を行う。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git clone git@github.com:imathis/octopress.git YoshihitoAso.github.io
</span><span class='line'>$ gem install bundler
</span><span class='line'>
</span><span class='line'>$ bundle install
</span><span class='line'>$ bundle exec rake install
</span><span class='line'>$ bundle exec rake setup_github_pages
</span><span class='line'> Enter the read/write url for your repository: git@github.com: username/username.github.com.git
</span><span class='line'> git@github.com:YoshihitoAso/YoshihitoAso.github.io.git</span></code></pre></td></tr></table></div></figure>


<h2>sample page</h2>

<p>インストールが終わったので、サンプルページを作ってみる。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ rake new_post['sample_page']</span></code></pre></td></tr></table></div></figure>


<p>でOK。すると、./source/_posts 配下に新しい *.markdown ファイルが出来ているはず。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ rake preview</span></code></pre></td></tr></table></div></figure>


<p>でプレビューを見れる。<a href="http://localhost:4000">http://localhost:4000</a> で見れる。</p>

<h2>テーマを変更する</h2>

<p>テーマを変更してみようかという気になる。
以下は<a href="https://github.com/bijumon/oct2">oct2テーマ</a>を適用する方法。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ git clone git@github.com:bijumon/oct2.git .themes/oct2
</span><span class='line'>$ bundle exec rake install['oct2']
</span><span class='line'>$ rake preview</span></code></pre></td></tr></table></div></figure>


<h2>GitHub Pages へ記事をアップ</h2>

<p>最後に記事をdeployする。</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>$ cd source
</span><span class='line'>$ rake gen_deploy</span></code></pre></td></tr></table></div></figure>


<p>とすればOK。最初は10分程経つと見れるようになるはず。</p>
]]></content>
  </entry>
  
</feed>
